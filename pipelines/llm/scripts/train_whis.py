#!/usr/bin/env python3
"""
Whis Cybersecurity LLM Fine-tuning Script
Generated by training pipeline setup
"""

import os
import json
import torch
from datetime import datetime
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, 
    TrainingArguments, Trainer, DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import Dataset
import sys

# Import configuration
sys.path.append('/home/jimmie/linkops-industries/SOAR-copilot/training')
from model_config import WhisModelConfig
from evaluation_metrics import CybersecurityEvaluator

def main():
    print("üõ°Ô∏è Starting Whis Cybersecurity LLM Fine-tuning")
    print(f"‚è∞ Start time: {datetime.now()}")
    
    # Load configuration
    config = WhisModelConfig()
    
    # Load training data
    with open('/home/jimmie/linkops-industries/SOAR-copilot/training/training_data/cybersec_training_data_latest.json', 'r') as f:
        training_data = json.load(f)
    
    # Prepare dataset
    formatted_examples = []
    for category, examples in training_data.items():
        for example in examples:
            formatted_text = f"""Below is an instruction that describes a cybersecurity task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{example['instruction']}

### Input:
{example['input']}

### Response:
{example['output']}"""
            formatted_examples.append({'text': formatted_text})
    
    print(f"üìä Loaded {len(formatted_examples)} training examples")
    
    # Initialize model and tokenizer
    print("ü§ó Loading model and tokenizer...")
    
    tokenizer = AutoTokenizer.from_pretrained(config.base_model_id)
    tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        config.base_model_id,
        quantization_config=config.get_bnb_config(),
        device_map="auto",
        torch_dtype=torch.float16
    )
    
    # Prepare for training
    model = prepare_model_for_kbit_training(model)
    model = get_peft_model(model, config.get_lora_config())
    
    # Tokenize data
    def tokenize_function(examples):
        tokens = tokenizer(
            examples['text'],
            truncation=True,
            padding=False,
            max_length=config.max_sequence_length,
            return_overflowing_tokens=False
        )
        tokens["labels"] = tokens["input_ids"].copy()
        return tokens
    
    dataset = Dataset.from_list(formatted_examples)
    train_test_split = dataset.train_test_split(test_size=0.1, seed=42)
    
    train_dataset = train_test_split['train'].map(
        tokenize_function, batched=True,
        remove_columns=train_test_split['train'].column_names
    )
    
    eval_dataset = train_test_split['test'].map(
        tokenize_function, batched=True,
        remove_columns=train_test_split['test'].column_names
    )
    
    # Setup trainer
    training_args = config.get_training_arguments()
    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator
    )
    
    # Train model
    print("üèãÔ∏è Starting training...")
    trainer.train()
    
    # Evaluate model
    print("üìä Evaluating model...")
    eval_results = trainer.evaluate()
    
    # Save model
    print("üíæ Saving model...")
    model.save_pretrained(config.output_dir)
    tokenizer.save_pretrained(config.output_dir)
    
    # Save results
    results = {
        "training_date": datetime.now().isoformat(),
        "config": config.__dict__,
        "eval_results": eval_results,
        "training_examples": len(formatted_examples)
    }
    
    with open(f"{config.output_dir}/training_results.json", "w") as f:
        json.dump(results, f, indent=2)
    
    print(f"‚úÖ Training complete! Model saved to {config.output_dir}")

if __name__ == "__main__":
    main()
