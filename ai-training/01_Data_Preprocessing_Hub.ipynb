{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔄 Data Preprocessing & Sanitization Hub\n",
    "**CORE PIPELINE #1** - Drop Zone for Raw Training Materials\n",
    "\n",
    "## 📍 Purpose\n",
    "- **Drop Zone**: Where you place raw Hugging Face datasets, external JSON files, etc.\n",
    "- **Sanitization**: Clean, validate, and format data for LLM/RAG pipelines\n",
    "- **Quality Gates**: Ensure data meets training standards\n",
    "- **Output Routing**: Send processed data to LLM, RAG, or Test pipelines\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Data Processing Environment\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# Pipeline Paths\n",
    "BASE_DIR = Path(\"/home/jimmie/linkops-industries/SOAR-copilot/ai-training\")\n",
    "DROP_ZONE = BASE_DIR / \"drop_zone\"  # 📥 Where you drop raw files\n",
    "PROCESSED = BASE_DIR / \"processed\"   # ✅ Clean, validated data\n",
    "QUARANTINE = BASE_DIR / \"quarantine\" # ⚠️ Failed validation\n",
    "\n",
    "# Create directories\n",
    "for path in [DROP_ZONE, PROCESSED, QUARANTINE]:\n",
    "    path.mkdir(exist_ok=True)\n",
    "    \n",
    "print(\"🚀 Data Processing Hub Initialized\")\n",
    "print(f\"📥 Drop Zone: {DROP_ZONE}\")\n",
    "print(f\"✅ Processed: {PROCESSED}\")\n",
    "print(f\"⚠️ Quarantine: {QUARANTINE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📥 Step 1: Scan Drop Zone for New Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_drop_zone() -> List[Dict[str, Any]]:\n",
    "    \"\"\"Scan drop zone for new training materials\"\"\"\n",
    "    files_found = []\n",
    "    \n",
    "    for file_path in DROP_ZONE.glob(\"**/*\"):\n",
    "        if file_path.is_file():\n",
    "            files_found.append({\n",
    "                \"name\": file_path.name,\n",
    "                \"path\": str(file_path),\n",
    "                \"size_mb\": round(file_path.stat().st_size / 1024 / 1024, 2),\n",
    "                \"extension\": file_path.suffix,\n",
    "                \"modified\": datetime.fromtimestamp(file_path.stat().st_mtime),\n",
    "                \"type\": classify_file_type(file_path)\n",
    "            })\n",
    "    \n",
    "    return files_found\n",
    "\n",
    "def classify_file_type(file_path: Path) -> str:\n",
    "    \"\"\"Classify file type for processing pipeline\"\"\"\n",
    "    ext = file_path.suffix.lower()\n",
    "    name = file_path.name.lower()\n",
    "    \n",
    "    if ext in ['.jsonl', '.json'] and any(x in name for x in ['train', 'instruct', 'chat']):\n",
    "        return 'llm_training_data'\n",
    "    elif ext in ['.md', '.txt'] and any(x in name for x in ['playbook', 'knowledge', 'documentation']):\n",
    "        return 'rag_knowledge'\n",
    "    elif ext in ['.csv', '.xlsx'] and 'test' in name:\n",
    "        return 'test_scenarios'\n",
    "    elif ext == '.zip' or 'dataset' in name:\n",
    "        return 'huggingface_dataset'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "# Scan for new files\n",
    "new_files = scan_drop_zone()\n",
    "print(f\"📊 Found {len(new_files)} files in drop zone:\")\n",
    "\n",
    "for file_info in new_files[:5]:  # Show first 5\n",
    "    print(f\"📄 {file_info['name']} ({file_info['size_mb']}MB) - {file_info['type']}\")\n",
    "    \n",
    "if len(new_files) > 5:\n",
    "    print(f\"... and {len(new_files) - 5} more files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧹 Step 2: Data Sanitization & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_llm_training_data(file_path: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Sanitize and validate LLM training data\"\"\"\n",
    "    result = {\n",
    "        \"status\": \"pending\",\n",
    "        \"errors\": [],\n",
    "        \"warnings\": [],\n",
    "        \"examples_processed\": 0,\n",
    "        \"output_path\": None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Load data\n",
    "        if file_path.suffix == '.jsonl':\n",
    "            examples = []\n",
    "            with open(file_path, 'r') as f:\n",
    "                for line_num, line in enumerate(f, 1):\n",
    "                    try:\n",
    "                        examples.append(json.loads(line.strip()))\n",
    "                    except json.JSONDecodeError:\n",
    "                        result[\"warnings\"].append(f\"Line {line_num}: Invalid JSON\")\n",
    "        else:\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                examples = data if isinstance(data, list) else [data]\n",
    "        \n",
    "        # Validate format\n",
    "        valid_examples = []\n",
    "        for i, example in enumerate(examples):\n",
    "            if validate_training_example(example):\n",
    "                valid_examples.append(sanitize_training_example(example))\n",
    "            else:\n",
    "                result[\"errors\"].append(f\"Example {i}: Invalid format\")\n",
    "        \n",
    "        # Save sanitized data\n",
    "        if valid_examples:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            output_file = PROCESSED / f\"llm_sanitized_{timestamp}.jsonl\"\n",
    "            \n",
    "            with open(output_file, 'w') as f:\n",
    "                for example in valid_examples:\n",
    "                    f.write(json.dumps(example) + '\\n')\n",
    "            \n",
    "            result.update({\n",
    "                \"status\": \"success\",\n",
    "                \"examples_processed\": len(valid_examples),\n",
    "                \"output_path\": str(output_file)\n",
    "            })\n",
    "        else:\n",
    "            result[\"status\"] = \"failed\"\n",
    "            result[\"errors\"].append(\"No valid examples found\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        result[\"status\"] = \"error\"\n",
    "        result[\"errors\"].append(str(e))\n",
    "    \n",
    "    return result\n",
    "\n",
    "def validate_training_example(example: Dict) -> bool:\n",
    "    \"\"\"Validate training example format\"\"\"\n",
    "    required_fields = ['instruction', 'response']\n",
    "    return all(field in example for field in required_fields)\n",
    "\n",
    "def sanitize_training_example(example: Dict) -> Dict:\n",
    "    \"\"\"Clean and standardize training example\"\"\"\n",
    "    sanitized = {\n",
    "        \"instruction\": example[\"instruction\"].strip(),\n",
    "        \"response\": example[\"response\"].strip() if isinstance(example[\"response\"], str) else json.dumps(example[\"response\"]),\n",
    "        \"category\": example.get(\"category\", \"general\"),\n",
    "        \"source\": example.get(\"source\", \"external\")\n",
    "    }\n",
    "    \n",
    "    # Remove PII and sensitive data\n",
    "    sanitized = remove_pii(sanitized)\n",
    "    \n",
    "    return sanitized\n",
    "\n",
    "def remove_pii(example: Dict) -> Dict:\n",
    "    \"\"\"Remove personally identifiable information\"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Patterns to replace\n",
    "    patterns = {\n",
    "        r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b': '192.168.1.100',  # IP addresses\n",
    "        r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b': 'user@company.com',  # Emails\n",
    "        r'\\b\\d{4}[-\\s]?\\d{4}[-\\s]?\\d{4}[-\\s]?\\d{4}\\b': 'XXXX-XXXX-XXXX-XXXX'  # Credit cards\n",
    "    }\n",
    "    \n",
    "    for field in ['instruction', 'response']:\n",
    "        if field in example:\n",
    "            text = example[field]\n",
    "            for pattern, replacement in patterns.items():\n",
    "                text = re.sub(pattern, replacement, text)\n",
    "            example[field] = text\n",
    "    \n",
    "    return example\n",
    "\n",
    "print(\"🧹 Data sanitization functions loaded\")\n",
    "print(\"Ready to process LLM training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔄 Step 3: Process All Files in Drop Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_drop_zone_files():\n",
    "    \"\"\"Process all files in drop zone\"\"\"\n",
    "    processing_report = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"files_processed\": [],\n",
    "        \"success_count\": 0,\n",
    "        \"error_count\": 0,\n",
    "        \"warning_count\": 0\n",
    "    }\n",
    "    \n",
    "    new_files = scan_drop_zone()\n",
    "    \n",
    "    for file_info in new_files:\n",
    "        print(f\"\\n🔄 Processing: {file_info['name']}\")\n",
    "        file_path = Path(file_info['path'])\n",
    "        \n",
    "        if file_info['type'] == 'llm_training_data':\n",
    "            result = sanitize_llm_training_data(file_path)\n",
    "        elif file_info['type'] == 'rag_knowledge':\n",
    "            result = sanitize_rag_knowledge(file_path)\n",
    "        elif file_info['type'] == 'test_scenarios':\n",
    "            result = sanitize_test_scenarios(file_path)\n",
    "        else:\n",
    "            result = {\"status\": \"skipped\", \"errors\": [\"Unknown file type\"]}\n",
    "        \n",
    "        # Update report\n",
    "        file_result = {\n",
    "            \"file\": file_info['name'],\n",
    "            \"type\": file_info['type'],\n",
    "            \"status\": result['status'],\n",
    "            \"errors\": result.get('errors', []),\n",
    "            \"warnings\": result.get('warnings', []),\n",
    "            \"output\": result.get('output_path')\n",
    "        }\n",
    "        \n",
    "        processing_report[\"files_processed\"].append(file_result)\n",
    "        \n",
    "        if result['status'] == 'success':\n",
    "            processing_report[\"success_count\"] += 1\n",
    "            print(f\"✅ Success: {result.get('examples_processed', 0)} examples processed\")\n",
    "            # Move original file to archive\n",
    "            archive_dir = DROP_ZONE / \"archive\"\n",
    "            archive_dir.mkdir(exist_ok=True)\n",
    "            shutil.move(str(file_path), str(archive_dir / file_path.name))\n",
    "        else:\n",
    "            processing_report[\"error_count\"] += 1\n",
    "            print(f\"❌ Failed: {'; '.join(result.get('errors', []))}\")\n",
    "            # Move to quarantine\n",
    "            shutil.move(str(file_path), str(QUARANTINE / file_path.name))\n",
    "        \n",
    "        if result.get('warnings'):\n",
    "            processing_report[\"warning_count\"] += len(result['warnings'])\n",
    "            print(f\"⚠️ Warnings: {'; '.join(result['warnings'])}\")\n",
    "    \n",
    "    # Save processing report\n",
    "    report_file = PROCESSED / f\"processing_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    with open(report_file, 'w') as f:\n",
    "        json.dump(processing_report, f, indent=2)\n",
    "    \n",
    "    return processing_report\n",
    "\n",
    "def sanitize_rag_knowledge(file_path: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Sanitize RAG knowledge files\"\"\"\n",
    "    # Placeholder for RAG sanitization\n",
    "    return {\"status\": \"success\", \"output_path\": \"rag_processed.json\"}\n",
    "\n",
    "def sanitize_test_scenarios(file_path: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Sanitize test scenario files\"\"\"\n",
    "    # Placeholder for test sanitization  \n",
    "    return {\"status\": \"success\", \"output_path\": \"test_processed.json\"}\n",
    "\n",
    "# Process all files\n",
    "if new_files:\n",
    "    print(f\"🚀 Processing {len(new_files)} files...\")\n",
    "    report = process_drop_zone_files()\n",
    "    \n",
    "    print(f\"\\n📊 PROCESSING COMPLETE\")\n",
    "    print(f\"✅ Success: {report['success_count']}\")\n",
    "    print(f\"❌ Errors: {report['error_count']}\")\n",
    "    print(f\"⚠️ Warnings: {report['warning_count']}\")\nelse:\n",
    "    print(\"📭 No files to process in drop zone\")\n",
    "    print(f\"💡 Drop your Hugging Face datasets, JSON files, or training data into:\")\n",
    "    print(f\"   {DROP_ZONE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 Step 4: Route Processed Data to Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_processed_data():\n",
    "    \"\"\"Route processed data to appropriate pipelines\"\"\"\n",
    "    \n",
    "    # LLM Pipeline routing\n",
    "    llm_files = list(PROCESSED.glob(\"llm_sanitized_*.jsonl\"))\n",
    "    if llm_files:\n",
    "        llm_dest = BASE_DIR / \"llm/data\"\n",
    "        llm_dest.mkdir(exist_ok=True)\n",
    "        \n",
    "        for file in llm_files:\n",
    "            dest_file = llm_dest / f\"processed_{file.name}\"\n",
    "            shutil.copy2(str(file), str(dest_file))\n",
    "            print(f\"📤 LLM: {file.name} → {dest_file}\")\n",
    "    \n",
    "    # RAG Pipeline routing\n",
    "    rag_files = list(PROCESSED.glob(\"rag_sanitized_*.json\"))\n",
    "    if rag_files:\n",
    "        rag_dest = BASE_DIR / \"rag/chunks\"\n",
    "        rag_dest.mkdir(exist_ok=True)\n",
    "        \n",
    "        for file in rag_files:\n",
    "            dest_file = rag_dest / f\"processed_{file.name}\"\n",
    "            shutil.copy2(str(file), str(dest_file))\n",
    "            print(f\"📤 RAG: {file.name} → {dest_file}\")\n",
    "    \n",
    "    # Test Pipeline routing\n",
    "    test_files = list(PROCESSED.glob(\"test_sanitized_*.json\"))\n",
    "    if test_files:\n",
    "        test_dest = BASE_DIR / \"testing/scenarios\"\n",
    "        test_dest.mkdir(exist_ok=True)\n",
    "        \n",
    "        for file in test_files:\n",
    "            dest_file = test_dest / f\"processed_{file.name}\"\n",
    "            shutil.copy2(str(file), str(dest_file))\n",
    "            print(f\"📤 TEST: {file.name} → {dest_file}\")\n",
    "    \n",
    "    total_routed = len(llm_files) + len(rag_files) + len(test_files)\n",
    "    print(f\"\\n📊 Routed {total_routed} files to pipelines\")\n",
    "    \n",
    "    return {\n",
    "        \"llm_files\": len(llm_files),\n",
    "        \"rag_files\": len(rag_files), \n",
    "        \"test_files\": len(test_files)\n",
    "    }\n",
    "\n",
    "# Route processed data\n",
    "routing_results = route_processed_data()\n",
    "print(\"\\n🎯 PROCESSING PIPELINE COMPLETE\")\n",
    "print(\"Next steps:\")\n",
    "print(\"📖 LLM Training: Open '02_LLM_Fine_Tuning_Pipeline.ipynb'\")\n",
    "print(\"📚 RAG Learning: Open '03_RAG_Knowledge_Pipeline.ipynb'\")\n",
    "print(\"🧪 Testing: Open '04_Testing_Validation_Pipeline.ipynb'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}