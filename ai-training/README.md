# ğŸ§  AI-Training: Machine Learning Workflows for WHIS SOAR-Copilot

**Production ML workflows across three tracks: fine-tuning, retrieval-augmented generation (RAG), and evaluation/serving.**

---

## ğŸ¯ **CORRECTED ML ARCHITECTURE**

We maintain a single top-level **`ai-training/`** directory that hosts our machine learning workflows across three tracks: **fine-tuning**, **retrieval-augmented generation (RAG)**, and **evaluation/serving**.

* **Model ("Whis")**: a base LLM from Hugging Face. We **fine-tune** Whis using **LoRA adapters** on curated, sanitized datasets. Fine-tuning produces **adapter weights**, not a new base model.
* **Data intake & sanitization**: all raw inputs (documents, transcripts, logs) pass through standardized **ingest â†’ sanitize â†’ normalize** steps (PII redaction, deduplication, chunking, schema validation).
* **RAG pipeline**: we embed documents into a **vector store** (e.g., Qdrant/Chroma) and configure a **retriever** that supplies relevant context to Whis **at inference time**. RAG **does not change model weights**; it augments prompts with retrieved context.
* **Evaluation**: we maintain static **benchmark suites** for both fine-tuned tasks and RAG quality (e.g., exact-match/F1 on task sets, **RAGAS** for retrieval faithfulness, latency/throughput SLOs). Evaluation is reproducible and independent from training.
* **Serving**: we version and serve Whis with selected LoRA adapters and connect it to the retriever for production inference. Config chooses **which adapter** and **which index** are active.
* **Experiment tracking**: all runs log **params, metrics, artifacts, and dataset/version hashes** (e.g., MLflow/W&B), enabling rollback and auditability.
* **MLOps/CI**: automated checks validate data schemas, generate SBOMs for dependencies, run eval suites on PRs, and gate promotion to staging/prod.

---

## ğŸ“ **CORRECTED DIRECTORY STRUCTURE**

```
ai-training/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ configs/                     # Configuration management
â”‚   â”œâ”€â”€ model.whis.yaml         # Base + adapter config (no secrets)
â”‚   â”œâ”€â”€ rag.yaml                # Retriever, chunking, embedder config
â”‚   â””â”€â”€ eval.yaml               # Benchmark suites, thresholds
â”œâ”€â”€ data/                       # Data governance & versioning
â”‚   â”œâ”€â”€ raw/                    # Immutable inputs
â”‚   â”œâ”€â”€ staging/                # Sanitized, deduped, chunked
â”‚   â””â”€â”€ datasets/               # Train/val/test splits (versioned)
â”œâ”€â”€ fine_tune/                  # Fine-tuning pipeline (SEPARATE from RAG)
â”‚   â”œâ”€â”€ lora_train.py          # Fine-tuning script
â”‚   â”œâ”€â”€ adapters/              # Produced LoRA weights (versioned)
â”‚   â””â”€â”€ experiments/           # MLflow or logs
â”œâ”€â”€ rag/                        # RAG pipeline (SEPARATE from fine-tuning)
â”‚   â”œâ”€â”€ embed.py               # Document embedding
â”‚   â”œâ”€â”€ index/                 # Vector store metadata (not secrets)
â”‚   â””â”€â”€ retriever/             # Retriever configs, prompts
â”œâ”€â”€ eval/                       # Evaluation pipeline (INDEPENDENT)
â”‚   â”œâ”€â”€ benchmarks/            # Golden sets, RAGAS configs
â”‚   â”œâ”€â”€ run_eval.py            # Produces reproducible reports
â”‚   â””â”€â”€ reports/               # Evaluation results
â”œâ”€â”€ serving/                    # Production serving
â”‚   â”œâ”€â”€ inference_server.py    # Loads base + adapter, connects retriever
â”‚   â””â”€â”€ router.py              # Chooses adapter/index per env
â””â”€â”€ scripts/                    # Shared utilities
    â”œâ”€â”€ ingest_sanitize.py     # Data intake and sanitization
    â”œâ”€â”€ make_splits.py         # Dataset splitting
    â””â”€â”€ export_artifacts.py    # Artifact management
```

---

## ğŸ”§ **TERMINOLOGY CORRECTIONS**

* **âŒ "RAG vectorizes data for Whis to use"**
  * **âœ… "RAG embeds documents into a vector store used by the retriever, which supplies context to Whis at inference"**

* **âŒ "Test pipeline trains until master"**
  * **âœ… "Evaluation pipeline measures performance on held-out benchmarks; training remains separate"**

* **âŒ "Learns with LoRA like Google Colab"**
  * **âœ… "Fine-tunes with LoRA adapters (as we previously prototyped in Colab)"**

---

## ğŸ¯ **MISSING COMPONENTS IDENTIFIED**

### **Data Governance**
- [ ] PII redaction policies
- [ ] License tracking  
- [ ] Dataset versioning (DVC or data registry)
- [ ] Lineage tracking (which model used which dataset)

### **Experiment Tracking**
- [ ] MLflow/W&B run logs
- [ ] Metrics and artifacts logging
- [ ] Commit hash + dataset version pinning

### **Clear Metrics**
- [ ] **Fine-tune**: accuracy/F1/ROUGE/BLEU, loss curves, calibration
- [ ] **RAG**: RAGAS (faithfulness, answer relevance, context precision/recall), retrieval hit@k, latency p95

### **Serving/Rollout**
- [ ] Canary adapters
- [ ] Shadow traffic
- [ ] Rollback plan

### **Guardrails**
- [ ] JSON schema validation for outputs
- [ ] Refusal policies
- [ ] Prompt-injection defenses for RAG

### **Resource Policy**
- [ ] GPU/VRAM budgeting
- [ ] Fallbacks (CPU)
- [ ] Device selection

---

## ğŸ›¡ï¸ **SECURITY CHECKLIST (ML/RAG Specific)**

### **Data Security**
- [ ] **Data leakage/PII**: sanitize + redact before embedding or training; store redaction logs
- [ ] **Prompt injection via RAG**: strip/neutralize instructions from retrieved docs; use system prompts and content-binding; cap context length
- [ ] **Model inversion risks**: avoid training on secrets; add canary phrases to detect leakage; refuse requests for verbatim training data

### **Supply Chain Security**
- [ ] Pin HF model + tokenizer versions; verify checksums
- [ ] Lock dependency versions; generate SBOM
- [ ] **Secrets**: keep API keys, DB creds in env/secret stores only; never in repo or artifacts

### **Production Security**
- [ ] No secrets in code/artifacts; all via env/secret store
- [ ] Data sanitize/redact enforced; datasets versioned
- [ ] Fine-tune and RAG separated; configs pinned
- [ ] Eval suites (incl. RAGAS) with promotion gates
- [ ] Dependency versions pinned; SBOM + scans documented
- [ ] Inference enforces output schema/guardrails
- [ ] Canary/rollback for adapter deployments

---

## ğŸš€ **EXECUTION PLAN (No Code Implementation)**

1. **Split pipelines**: create `fine_tune/` and `rag/` with distinct entry points and configs
2. **Data contract**: define a `Record` schema (source, text, labels, redaction flags) and a `Chunk` schema (id, text, meta). Enforce in sanitize step
3. **Version everything**: dataset hashes; adapter versions; index versions; config snapshots
4. **Eval first**: lock a small golden set + RAGAS suite; define pass/fail gates (e.g., RAGAS â‰¥ 0.75, p95 latency â‰¤ 3s)
5. **Serving plan**: one command loads base + chosen adapter + retriever; support canary adapter via config
6. **CI hooks**: on PR, run sanitize validation, tiny training smoke (1 epoch), RAGAS sample eval, and security scans

---

**Status**: Architecture corrected according to mentor feedback. Ready for proper ML pipeline implementation.