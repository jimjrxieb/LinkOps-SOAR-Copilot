#!/usr/bin/env python3
"""
üß† Cybersecurity Knowledge Base Builder
======================================
Ingests multiple Hugging Face cybersecurity datasets into RAG pipeline

[TAG: RAG-REBUILD] - Complete knowledge base reconstruction
[TAG: DATASET-INGESTION] - Multi-source cybersecurity data
[TAG: FAISS-VECTOR] - Production vector store creation
"""

import os
import json
import pandas as pd
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import logging
from tqdm import tqdm
import hashlib

# HuggingFace datasets
try:
    from datasets import load_dataset
    HF_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è  datasets library not available. Install with: pip install datasets")
    HF_AVAILABLE = False

# Sentence transformers for embeddings
try:
    from sentence_transformers import SentenceTransformer
    ST_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è  sentence-transformers not available. Install with: pip install sentence-transformers")
    ST_AVAILABLE = False

# FAISS for vector storage
try:
    import faiss
    import numpy as np
    FAISS_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è  FAISS not available. Install with: pip install faiss-cpu")
    FAISS_AVAILABLE = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CybersecurityKnowledgeBuilder:
    """
    Build comprehensive cybersecurity knowledge base from multiple HF datasets
    
    [TAG: RAG-REBUILD] - Multi-dataset ingestion pipeline
    """
    
    def __init__(self, output_dir: str = "ai-training/rag/vectorstore"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.chunks_dir = Path("ai-training/rag/chunks")
        self.chunks_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize embeddings model
        self.embeddings_model = None
        if ST_AVAILABLE:
            print("üöÄ Loading sentence transformer model...")
            self.embeddings_model = SentenceTransformer('all-MiniLM-L6-v2')
        
        # Knowledge base metadata
        self.knowledge_base = {
            "version": "1.0.0",
            "created_at": datetime.utcnow().isoformat(),
            "datasets": [],
            "chunk_count": 0,
            "categories": {}
        }
        
        self.chunks = []
        self.embeddings = []
    
    def create_base_glossary(self):
        """Create fundamental cybersecurity glossary entries"""
        
        base_knowledge = {
            "siem": {
                "term": "SIEM",
                "full_form": "Security Information and Event Management", 
                "definition": "A SIEM system combines security information management (SIM) and security event management (SEM) to provide real-time analysis of security alerts generated by applications and network hardware.",
                "examples": ["Splunk", "IBM QRadar", "ArcSight", "LogRhythm"],
                "category": "security_tools"
            },
            "edr": {
                "term": "EDR",
                "full_form": "Endpoint Detection and Response",
                "definition": "EDR solutions monitor and collect activity data from endpoints that could indicate a threat. They provide continuous monitoring, threat detection, and response capabilities.",
                "examples": ["LimaCharlie", "CrowdStrike Falcon", "SentinelOne", "Microsoft Defender ATP"],
                "category": "security_tools"
            },
            "soar": {
                "term": "SOAR", 
                "full_form": "Security Orchestration, Automation & Response",
                "definition": "SOAR platforms combine incident response, orchestration and automation, and threat intelligence management capabilities in a single solution.",
                "examples": ["Phantom", "Demisto", "Resilient", "WHIS"],
                "category": "security_platforms"
            },
            "cia_triad": {
                "term": "CIA Triad",
                "full_form": "Confidentiality, Integrity, Availability",
                "definition": "The CIA triad is a foundational model in cybersecurity that guides policies for information security within an organization. Confidentiality ensures data is accessible only to authorized users, Integrity ensures data accuracy and trustworthiness, and Availability ensures authorized users have access when needed.",
                "category": "security_principles"
            },
            "cybersecurity": {
                "term": "Cybersecurity",
                "definition": "The practice of protecting systems, networks, and programs from digital attacks. These cyberattacks are usually aimed at accessing, changing, or destroying sensitive information; extorting money from users; or interrupting normal business processes.",
                "category": "fundamentals"
            },
            "incident_response": {
                "term": "Incident Response",
                "definition": "A structured methodology for handling security breaches or cyberattacks. It includes preparation, identification, containment, eradication, recovery, and lessons learned phases.",
                "category": "processes"
            },
            "threat_hunting": {
                "term": "Threat Hunting", 
                "definition": "The practice of proactively searching through networks and datasets to detect and isolate advanced threats that evade existing security solutions.",
                "category": "processes"
            },
            "mitre_attack": {
                "term": "MITRE ATT&CK",
                "definition": "A globally-accessible knowledge base of adversary tactics, techniques, and procedures (TTPs) based on real-world observations of cyberattacks.",
                "category": "frameworks"
            },
            
            # === KUBERNETES SECURITY ===
            "cks": {
                "term": "CKS",
                "full_form": "Certified Kubernetes Security Specialist",
                "definition": "CKS is a performance-based certification that tests a candidate's ability to secure container-based applications and Kubernetes platforms during build, deployment and runtime.",
                "examples": ["Cluster Setup", "Pod Security Standards", "Supply Chain Security", "Runtime Security"],
                "category": "certifications"
            },
            "ccsp": {
                "term": "CCSP",
                "full_form": "Certified Cloud Security Professional",
                "definition": "CCSP is an advanced certification that demonstrates expertise in cloud security architecture, governance, risk management, and compliance across major cloud platforms.",
                "examples": ["Cloud Architecture", "Data Security", "Legal & Compliance", "Application Security"],
                "category": "certifications"
            },
            "kubernetes_rbac": {
                "term": "Kubernetes RBAC",
                "full_form": "Role-Based Access Control",
                "definition": "Kubernetes RBAC is a method of regulating access to computer or network resources based on the roles of individual users within an enterprise cluster.",
                "examples": ["ServiceAccounts", "ClusterRoles", "RoleBindings", "ClusterRoleBindings"],
                "category": "kubernetes_security"
            },
            "pod_security": {
                "term": "Pod Security Standards",
                "full_form": "Pod Security Standards",
                "definition": "Pod Security Standards define security policies for pod specifications to prevent privilege escalation and enforce security baselines.",
                "examples": ["Privileged", "Baseline", "Restricted", "SecurityContext"],
                "category": "kubernetes_security"
            },
            "network_policy": {
                "term": "Kubernetes Network Policy",
                "full_form": "Kubernetes Network Policy",
                "definition": "Network Policies are specifications of how groups of pods are allowed to communicate with each other and other network endpoints in Kubernetes.",
                "examples": ["Ingress Rules", "Egress Rules", "Pod Selectors", "Namespace Selectors"],
                "category": "kubernetes_security"
            },
            
            # === CLOUD SECURITY ===
            "aws_cloudtrail": {
                "term": "AWS CloudTrail",
                "full_form": "Amazon Web Services CloudTrail",
                "definition": "AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account by logging API calls.",
                "examples": ["Event History", "Insights", "Data Events", "Management Events"],
                "category": "aws_security"
            },
            "azure_security_center": {
                "term": "Azure Security Center",
                "full_form": "Microsoft Azure Security Center",
                "definition": "Azure Security Center is a unified infrastructure security management system that strengthens the security posture of your data centers.",
                "examples": ["Secure Score", "Recommendations", "Threat Protection", "Compliance Dashboard"],
                "category": "azure_security"
            },
            "shared_responsibility": {
                "term": "Cloud Shared Responsibility Model",
                "full_form": "Cloud Shared Responsibility Model",
                "definition": "The shared responsibility model defines which security tasks are handled by the cloud provider and which tasks are handled by the customer.",
                "examples": ["Infrastructure Security", "Data Classification", "Identity Management", "Network Controls"],
                "category": "cloud_fundamentals"
            },
            
            # === COMPLIANCE FRAMEWORKS ===
            "soc2": {
                "term": "SOC 2",
                "full_form": "Service Organization Control 2",
                "definition": "SOC 2 is an auditing procedure that ensures service providers securely manage data to protect the interests of clients and their customers.",
                "examples": ["Security", "Availability", "Processing Integrity", "Confidentiality"],
                "category": "compliance"
            },
            "gdpr": {
                "term": "GDPR",
                "full_form": "General Data Protection Regulation",
                "definition": "GDPR is a legal framework that sets guidelines for the collection and processing of personal information of individuals within the European Union.",
                "examples": ["Data Subject Rights", "Breach Notification", "Privacy by Design", "Data Protection Officer"],
                "category": "privacy_regulations"
            },
            "pci_dss": {
                "term": "PCI DSS",
                "full_form": "Payment Card Industry Data Security Standard",
                "definition": "PCI DSS is an information security standard for organizations that handle branded credit cards from major card schemes.",
                "examples": ["Network Security", "Cardholder Data Protection", "Vulnerability Management", "Access Control"],
                "category": "compliance"
            },
            "iso27001": {
                "term": "ISO 27001",
                "full_form": "International Organization for Standardization 27001",
                "definition": "ISO 27001 is the international standard that describes how to manage information security in a company using an Information Security Management System (ISMS).",
                "examples": ["Risk Assessment", "Security Controls", "Management Review", "Continuous Improvement"],
                "category": "compliance"
            },
            "nist_framework": {
                "term": "NIST Cybersecurity Framework",
                "full_form": "National Institute of Standards and Technology Cybersecurity Framework",
                "definition": "The NIST Framework provides a policy framework of computer security guidance for how private sector organizations can assess and improve their ability to prevent, detect, and respond to cyber attacks.",
                "examples": ["Identify", "Protect", "Detect", "Respond", "Recover"],
                "category": "frameworks"
            },
            
            # === VENDOR-SPECIFIC TOOLS ===
            "guardpoint": {
                "term": "Guardpoint",
                "full_form": "Guardpoint Cybersecurity",
                "definition": "Guardpoint is a managed security services provider (MSSP) that offers 24/7 security operations center services, threat hunting, incident response, and cybersecurity consulting.",
                "examples": ["SOC Services", "Threat Hunting", "Incident Response", "Security Consulting"],
                "category": "managed_security"
            },
            "crowdstrike": {
                "term": "CrowdStrike Falcon",
                "full_form": "CrowdStrike Falcon Platform",
                "definition": "CrowdStrike Falcon is a cloud-native endpoint protection platform that combines next-generation antivirus, endpoint detection and response, and threat intelligence.",
                "examples": ["Threat Graph", "Real-time Response", "Threat Hunting", "Intelligence"],
                "category": "edr_platforms"
            },
            "microsoft_sentinel": {
                "term": "Microsoft Sentinel",
                "full_form": "Microsoft Sentinel SIEM",
                "definition": "Microsoft Sentinel is a cloud-native security information and event management (SIEM) and security orchestration, automation, and response (SOAR) solution.",
                "examples": ["Data Connectors", "Analytics Rules", "Workbooks", "Playbooks"],
                "category": "cloud_siem"
            },
            "splunk_es": {
                "term": "Splunk Enterprise Security", 
                "full_form": "Splunk Enterprise Security",
                "definition": "Splunk Enterprise Security is a premium security information and event management solution that provides security teams with insight into machine data generated from security technologies.",
                "examples": ["Correlation Searches", "Notable Events", "Risk-Based Alerting", "Security Domains"],
                "category": "siem_platforms"
            },
            
            # === ADVANCED SECURITY CONCEPTS ===
            "zero_trust": {
                "term": "Zero Trust",
                "full_form": "Zero Trust Architecture",
                "definition": "Zero Trust is a security concept centered on the belief that organizations should not automatically trust anything inside or outside its perimeters and instead must verify anything and everything trying to connect to its systems.",
                "examples": ["Never Trust, Always Verify", "Least Privilege Access", "Micro-segmentation", "Multi-factor Authentication"],
                "category": "security_architecture"
            },
            "threat_intelligence": {
                "term": "Threat Intelligence",
                "full_form": "Cyber Threat Intelligence",
                "definition": "Threat intelligence is evidence-based knowledge about existing or emerging threats that can be used to inform decisions regarding the subject's response to those threats.",
                "examples": ["Strategic Intelligence", "Tactical Intelligence", "Operational Intelligence", "Technical Intelligence"],
                "category": "threat_analysis"
            },
            "supply_chain_attack": {
                "term": "Supply Chain Attack",
                "full_form": "Supply Chain Cyber Attack", 
                "definition": "A supply chain attack is a cyber attack that seeks to damage an organization by targeting less-secure elements in the supply network.",
                "examples": ["SolarWinds", "Third-party Vendors", "Software Dependencies", "Hardware Tampering"],
                "category": "attack_vectors"
            },
            "apt": {
                "term": "APT",
                "full_form": "Advanced Persistent Threat",
                "definition": "An APT is a stealthy computer network attack in which a person or group gains unauthorized access to a network and remains undetected for an extended period.",
                "examples": ["Nation-state Actors", "Long-term Campaigns", "Lateral Movement", "Data Exfiltration"],
                "category": "threat_actors"
            },
            "ransomware": {
                "term": "Ransomware",
                "full_form": "Ransomware",
                "definition": "Ransomware is a type of malicious software designed to block access to a computer system until a sum of money is paid.",
                "examples": ["WannaCry", "Ryuk", "Maze", "REvil"],
                "category": "malware"
            },
            "ids": {
                "term": "IDS",
                "full_form": "Intrusion Detection System",
                "definition": "An IDS is a device or software application that monitors a network or systems for malicious activity or policy violations.",
                "examples": ["Network-based IDS", "Host-based IDS", "Signature-based Detection", "Anomaly-based Detection"],
                "category": "network_security"
            },
            "ips": {
                "term": "IPS",
                "full_form": "Intrusion Prevention System",
                "definition": "An IPS is a network security tool that continuously monitors a network for malicious activity and takes action to prevent it.",
                "examples": ["Inline Deployment", "Active Response", "Threat Blocking", "Real-time Prevention"],
                "category": "network_security"
            },
            "soc": {
                "term": "SOC",
                "full_form": "Security Operations Center",
                "definition": "A SOC is a centralized unit that deals with security issues on an organizational and technical level, providing 24/7 monitoring and analysis of security events.",
                "examples": ["Threat Monitoring", "Incident Response", "Security Analytics", "Threat Hunting"],
                "category": "security_operations"
            },
            "vulnerability_management": {
                "term": "Vulnerability Management",
                "full_form": "Vulnerability Management",
                "definition": "Vulnerability management is the process of identifying, evaluating, treating, and reporting on security vulnerabilities in systems and software.",
                "examples": ["Vulnerability Scanning", "Risk Assessment", "Patch Management", "Remediation Planning"],
                "category": "risk_management"
            },
            "defense_in_depth": {
                "term": "Defense in Depth",
                "full_form": "Defense in Depth Strategy",
                "definition": "Defense in depth is a security strategy that employs multiple layers of security controls throughout an information system to provide redundancy in case any single control fails.",
                "examples": ["Network Segmentation", "Access Controls", "Endpoint Protection", "Security Monitoring"],
                "category": "security_strategy"
            },
            "social_engineering": {
                "term": "Social Engineering",
                "full_form": "Social Engineering Attack",
                "definition": "Social engineering is the use of deception to manipulate individuals into divulging confidential or personal information that may be used for fraudulent purposes.",
                "examples": ["Phishing", "Pretexting", "Baiting", "Tailgating"],
                "category": "attack_vectors"
            }
        }
        
        # Create glossary chunks
        glossary_dir = self.chunks_dir / "core_glossary"
        glossary_dir.mkdir(exist_ok=True)
        
        for key, entry in base_knowledge.items():
            # Create detailed chunk
            chunk_content = f"# {entry['term']}\n\n"
            
            if 'full_form' in entry:
                chunk_content += f"**Full Form**: {entry['full_form']}\n\n"
            
            chunk_content += f"**Definition**: {entry['definition']}\n\n"
            
            if 'examples' in entry:
                chunk_content += f"**Examples**: {', '.join(entry['examples'])}\n\n"
            
            chunk_content += f"**Category**: {entry['category']}\n"
            
            # Write to file
            chunk_file = glossary_dir / f"{key}.md"
            with open(chunk_file, 'w') as f:
                f.write(chunk_content)
            
            # Add to chunks for embedding
            self.chunks.append({
                "id": f"glossary_{key}",
                "content": chunk_content,
                "title": entry['term'],
                "source": "core_glossary",
                "category": entry['category'],
                "type": "definition"
            })
        
        logger.info(f"‚úÖ Created {len(base_knowledge)} core glossary entries")
        return len(base_knowledge)
    
    def ingest_huggingface_datasets(self):
        """Ingest priority cybersecurity datasets from Hugging Face"""
        
        if not HF_AVAILABLE:
            logger.warning("Skipping HF dataset ingestion - datasets library not available")
            return 0
        
        # Priority datasets for WHIS knowledge
        datasets_config = [
            {
                "name": "trendmicro-ailab/Primus-Seed",
                "description": "Large curated cybersecurity corpus",
                "sample_size": 1000,  # Limit for demo
                "text_column": "text",
                "category": "general_security"
            },
            {
                "name": "Zainabsa99/mitre_attack",
                "description": "MITRE ATT&CK techniques",
                "sample_size": 500,
                "text_column": "description", 
                "category": "mitre_attack"
            },
            {
                "name": "zeroshot/cybersecurity-corpus",
                "description": "Broad security text collection",
                "sample_size": 800,
                "text_column": "text",
                "category": "cybersecurity_general"
            },
            {
                "name": "0xyf/windows-log-QnA",
                "description": "Windows event log Q&A pairs", 
                "sample_size": 300,
                "text_column": "answer",
                "category": "forensics_logs"
            }
        ]
        
        total_ingested = 0
        
        for dataset_config in datasets_config:
            try:
                logger.info(f"üì• Loading {dataset_config['name']}...")
                
                # Load dataset
                dataset = load_dataset(dataset_config["name"], split="train", streaming=True)
                
                # Sample and process
                count = 0
                for example in dataset:
                    if count >= dataset_config["sample_size"]:
                        break
                    
                    # Extract text content
                    text_content = ""
                    if dataset_config["text_column"] in example:
                        text_content = str(example[dataset_config["text_column"]])
                    elif "text" in example:
                        text_content = str(example["text"])
                    elif "content" in example:
                        text_content = str(example["content"])
                    else:
                        # Try to find any text field
                        text_fields = [k for k, v in example.items() if isinstance(v, str) and len(v) > 50]
                        if text_fields:
                            text_content = str(example[text_fields[0]])
                    
                    if len(text_content) > 100:  # Minimum content length
                        chunk_id = f"hf_{dataset_config['name'].replace('/', '_')}_{count}"
                        
                        self.chunks.append({
                            "id": chunk_id,
                            "content": text_content[:2000],  # Limit chunk size
                            "title": f"Cybersecurity Knowledge - {count}",
                            "source": dataset_config["name"],
                            "category": dataset_config["category"],
                            "type": "hf_dataset"
                        })
                        
                        count += 1
                
                total_ingested += count
                logger.info(f"‚úÖ Ingested {count} chunks from {dataset_config['name']}")
                
                # Track in metadata
                self.knowledge_base["datasets"].append({
                    "name": dataset_config["name"],
                    "description": dataset_config["description"],
                    "chunks_ingested": count,
                    "category": dataset_config["category"]
                })
                
            except Exception as e:
                logger.error(f"‚ùå Failed to load {dataset_config['name']}: {e}")
                continue
        
        return total_ingested
    
    def ingest_open_malsec_data(self):
        """Ingest Open-MalSec threat intelligence data for enhanced malware/phishing knowledge"""
        
        import json
        import glob
        
        malsec_dir = Path("open-malsec")
        if not malsec_dir.exists():
            logger.warning("Open-MalSec directory not found - skipping threat intelligence ingestion")
            return 0
        
        total_ingested = 0
        
        # Find all JSON files in open-malsec directory
        json_files = list(malsec_dir.glob("*.json"))
        logger.info(f"Found {len(json_files)} Open-MalSec JSON files")
        
        for json_file in json_files:
            try:
                logger.info(f"üì• Processing {json_file.name}...")
                
                with open(json_file, 'r') as f:
                    data = json.load(f)
                
                # Handle different data structures
                if isinstance(data, list):
                    threat_data = data
                elif isinstance(data, dict) and 'data' in data:
                    threat_data = data['data']
                else:
                    threat_data = [data]
                
                # Process each threat intelligence entry
                for i, entry in enumerate(threat_data[:100]):  # Limit to 100 per file
                    if not isinstance(entry, dict):
                        continue
                    
                    # Extract threat intelligence content
                    instruction = entry.get('Instruction', '')
                    input_text = entry.get('Input', '')
                    output_text = entry.get('Output', '')
                    metadata = entry.get('Metadata', {})
                    threat_type = metadata.get('threat_type', json_file.stem.replace('-', '_'))
                    
                    # Create knowledge chunk
                    chunk_content = f"# Threat Intelligence: {threat_type.title()}\n\n"
                    
                    if instruction:
                        chunk_content += f"**Analysis Task**: {instruction}\n\n"
                    
                    if input_text:
                        chunk_content += f"**Threat Indicator**: {input_text}\n\n"
                    
                    if output_text:
                        chunk_content += f"**Analysis Result**: {output_text}\n\n"
                    
                    if metadata:
                        chunk_content += f"**Threat Metadata**: {json.dumps(metadata, indent=2)}\n"
                    
                    # Generate unique chunk ID
                    chunk_id = f"malsec_{json_file.stem}_{i}"
                    
                    # Add to chunks
                    self.chunks.append({
                        "id": chunk_id,
                        "content": chunk_content,
                        "title": f"Threat Intelligence: {threat_type.title()}",
                        "source": "open_malsec",
                        "category": "threat_intelligence",
                        "threat_type": threat_type,
                        "type": "threat_analysis"
                    })
                    
                    total_ingested += 1
                
                logger.info(f"‚úÖ Ingested {min(len(threat_data), 100)} threat intelligence entries from {json_file.name}")
                
            except Exception as e:
                logger.error(f"‚ùå Failed to process {json_file.name}: {e}")
                continue
        
        # Also ingest README content for context
        readme_file = malsec_dir / "README.md"
        if readme_file.exists():
            try:
                with open(readme_file, 'r') as f:
                    readme_content = f.read()
                
                self.chunks.append({
                    "id": "malsec_overview",
                    "content": readme_content,
                    "title": "Open-MalSec Dataset Overview",
                    "source": "open_malsec",
                    "category": "dataset_documentation",
                    "type": "reference"
                })
                
                total_ingested += 1
                logger.info("‚úÖ Added Open-MalSec dataset overview")
                
            except Exception as e:
                logger.warning(f"Could not read Open-MalSec README: {e}")
        
        return total_ingested

    def ingest_forensics_data(self):
        """
        Ingest forensics and SIEM datasets for enhanced log analysis knowledge
        """
        logger.info("üîç Ingesting forensics and SIEM data...")
        
        forensics_dir = Path("ai-training/rag/forensics-data")
        
        if not forensics_dir.exists():
            logger.warning(f"Forensics directory not found: {forensics_dir}")
            return 0
        
        total_processed = 0
        
        # Process witfoo syslog data
        witfoo_file = forensics_dir / "witfoo_syslog_samples.jsonl"
        if witfoo_file.exists():
            logger.info("Processing witfoo syslog samples...")
            
            try:
                with open(witfoo_file, 'r') as f:
                    for i, line in enumerate(f):
                        if i >= 200:  # Limit to avoid overwhelming
                            break
                            
                        entry = json.loads(line.strip())
                        syslog = entry.get('syslog', '')
                        artifact = entry.get('artifact', '')
                        
                        if not all([syslog, artifact]):
                            continue
                        
                        # Create structured content
                        chunk_content = f"# SIEM Log Analysis\n\n"
                        chunk_content += f"**Log Entry**: {syslog}\n\n"
                        chunk_content += f"**Security Artifact**: {artifact}\n\n"
                        chunk_content += f"**Analysis**: This syslog entry indicates {artifact.lower()} activity that security analysts should investigate.\n\n"
                        chunk_content += f"**Source**: Witfoo SIEM Dataset\n"
                        chunk_content += f"**Category**: Log Analysis\n"
                        
                        # Create chunk
                        chunk_id = f"witfoo_syslog_{i}"
                        chunk_data = {
                            'id': chunk_id,
                            'content': chunk_content,
                            'source': 'witfoo_syslog',
                            'title': f"SIEM Log Analysis: {artifact}",
                            'category': 'log_analysis',
                            'metadata': {
                                'artifact_type': artifact.lower(),
                                'log_source': 'syslog'
                            }
                        }
                        
                        self.chunks.append(chunk_data)
                        total_processed += 1
                        
            except Exception as e:
                logger.error(f"Error processing witfoo data: {e}")
        
        # Process hayabusa forensic reasoning data
        hayabusa_file = forensics_dir / "hayabusa_forensic_samples.jsonl"
        if hayabusa_file.exists():
            logger.info("Processing hayabusa forensic reasoning samples...")
            
            try:
                with open(hayabusa_file, 'r') as f:
                    for i, line in enumerate(f):
                        if i >= 100:  # Limit to avoid overwhelming
                            break
                            
                        entry = json.loads(line.strip())
                        question = entry.get('question', '')
                        answer = entry.get('answer', '')
                        context = entry.get('context', '')
                        
                        if not all([question, answer]):
                            continue
                        
                        # Create structured content
                        chunk_content = f"# Digital Forensics Reasoning\n\n"
                        chunk_content += f"**Forensic Question**: {question}\n\n"
                        chunk_content += f"**Expert Analysis**: {answer}\n\n"
                        if context:
                            chunk_content += f"**Context**: {context}\n\n"
                        chunk_content += f"**Source**: Hayabusa Forensic Reasoning Dataset\n"
                        chunk_content += f"**Category**: Digital Forensics\n"
                        
                        # Create chunk
                        chunk_id = f"hayabusa_forensics_{i}"
                        chunk_data = {
                            'id': chunk_id,
                            'content': chunk_content,
                            'source': 'hayabusa_forensics',
                            'title': "Digital Forensics Analysis",
                            'category': 'forensics',
                            'metadata': {
                                'analysis_type': 'forensic_reasoning'
                            }
                        }
                        
                        self.chunks.append(chunk_data)
                        total_processed += 1
                        
            except Exception as e:
                logger.error(f"Error processing hayabusa data: {e}")
        
        logger.info(f"‚úÖ Processed {total_processed} forensics and SIEM entries")
        return total_processed

    def ingest_soar_datasets(self):
        """
        Ingest SOAR datasets for elite incident response and security operations
        """
        logger.info("üöÄ Ingesting elite SOAR datasets...")
        
        soar_dir = Path("ai-training/rag/soar-data")
        
        if not soar_dir.exists():
            logger.warning(f"SOAR directory not found: {soar_dir}")
            return 0
        
        total_processed = 0
        
        # Process incident response playbooks
        ir_dir = soar_dir / "ir_playbooks_extended"
        if ir_dir.exists():
            logger.info("Processing incident response playbooks...")
            
            for jsonl_file in ir_dir.glob("*_samples.jsonl"):
                try:
                    with open(jsonl_file, 'r') as f:
                        for i, line in enumerate(f):
                            if i >= 100:  # Limit to avoid overwhelming
                                break
                                
                            entry = json.loads(line.strip())
                            playbook_content = entry.get('playbook_content', '')
                            incident_type = entry.get('incident_type', 'General Incident')
                            
                            if not playbook_content or len(playbook_content) < 100:
                                continue
                            
                            # Clean up the content (remove training tokens)
                            content = playbook_content.replace('<s>[INST]', '').replace('[/INST]', '').replace('</s>', '')
                            content = content.replace('<<SYS>>', '').replace('<</SYS>>', '')
                            
                            # Create structured content
                            chunk_content = f"# Incident Response Playbook\n\n"
                            chunk_content += f"**Incident Type**: {incident_type}\n\n"
                            chunk_content += f"**Framework**: NIST SP 800-61 Rev 2\n\n"
                            chunk_content += f"**Playbook Content**:\n{content}\n\n"
                            chunk_content += f"**Category**: Incident Response Playbook\n"
                            chunk_content += f"**Source**: SOAR Elite Dataset\n"
                            
                            # Create chunk
                            chunk_id = f"soar_ir_{jsonl_file.stem}_{i}"
                            chunk_data = {
                                'id': chunk_id,
                                'content': chunk_content,
                                'source': 'soar_ir_playbooks',
                                'title': f"IR Playbook: {incident_type}",
                                'category': 'incident_response',
                                'metadata': {
                                    'playbook_type': 'incident_response',
                                    'framework': 'nist',
                                    'incident_type': incident_type.lower()
                                }
                            }
                            
                            self.chunks.append(chunk_data)
                            total_processed += 1
                            
                except Exception as e:
                    logger.error(f"Error processing {jsonl_file}: {e}")
        
        # Process security DPO instructions
        dpo_dir = soar_dir / "security_dpo"
        if dpo_dir.exists():
            logger.info("Processing security DPO instructions...")
            
            for jsonl_file in dpo_dir.glob("*_samples.jsonl"):
                try:
                    with open(jsonl_file, 'r') as f:
                        for i, line in enumerate(f):
                            if i >= 200:  # Limit to avoid overwhelming
                                break
                                
                            entry = json.loads(line.strip())
                            instruction = entry.get('instruction', '')
                            response = entry.get('response', '')
                            
                            if not all([instruction, response]) or len(response) < 50:
                                continue
                            
                            # Create structured content
                            chunk_content = f"# Security Operations Instruction\n\n"
                            chunk_content += f"**Instruction**: {instruction}\n\n"
                            chunk_content += f"**Response**: {response}\n\n"
                            chunk_content += f"**Category**: Security Operations\n"
                            chunk_content += f"**Source**: Security DPO Dataset\n"
                            
                            # Create chunk
                            chunk_id = f"soar_dpo_{jsonl_file.stem}_{i}"
                            chunk_data = {
                                'id': chunk_id,
                                'content': chunk_content,
                                'source': 'security_dpo',
                                'title': "Security Operations Guidance",
                                'category': 'security_operations',
                                'metadata': {
                                    'instruction_type': 'security_operations',
                                    'framework': 'dpo'
                                }
                            }
                            
                            self.chunks.append(chunk_data)
                            total_processed += 1
                            
                except Exception as e:
                    logger.error(f"Error processing {jsonl_file}: {e}")
        
        logger.info(f"‚úÖ Processed {total_processed} SOAR datasets entries")
        return total_processed
    
    def add_local_security_content(self):
        """Add local security content and configurations"""
        
        # WHIS system configuration knowledge
        system_config = {
            "siem_system": "Splunk",
            "edr_system": "LimaCharlie", 
            "soar_platform": "WHIS",
            "orchestration": "Slack integration",
            "automation": "Playwright testing"
        }
        
        config_content = """# WHIS System Configuration

## Current Security Stack

**SIEM System**: Splunk
- Real-time log analysis and correlation
- Custom detection rules and alerting
- Integration with WHIS for automated response

**EDR System**: LimaCharlie  
- Endpoint monitoring and response
- Real-time telemetry streaming
- Automated sensor isolation capabilities

**SOAR Platform**: WHIS (Workforce Hybrid Intelligence System)
- AI-powered security operations copilot
- Automated playbook execution
- Human-in-the-loop approval workflows

**Orchestration**: Slack Integration
- Real-time alert notifications
- Interactive response workflows
- Team collaboration and approval processes

**Testing Framework**: Playwright Automation
- Automated security testing in sandboxed environments
- Website security validation
- Penetration testing automation

## Operational Procedures

- All high-risk actions require human approval
- PII is automatically redacted from all communications
- Knowledge gaps are tracked and used for continuous learning
- Security events are logged with full audit trail
"""

        self.chunks.append({
            "id": "whis_system_config",
            "content": config_content,
            "title": "WHIS System Configuration",
            "source": "system_config",
            "category": "configuration",
            "type": "system_knowledge"
        })
        
        # Save config file
        config_file = self.chunks_dir / "system_configuration.md"
        with open(config_file, 'w') as f:
            f.write(config_content)
        
        logger.info("‚úÖ Added WHIS system configuration knowledge")
        return 1
    
    def create_embeddings(self):
        """Create embeddings for all chunks"""
        
        if not self.embeddings_model:
            logger.error("‚ùå No embeddings model available")
            return False
        
        logger.info(f"üßÆ Creating embeddings for {len(self.chunks)} chunks...")
        
        # Extract text content
        texts = [chunk["content"] for chunk in self.chunks]
        
        # Create embeddings in batches
        batch_size = 32
        embeddings = []
        
        for i in tqdm(range(0, len(texts), batch_size), desc="Creating embeddings"):
            batch = texts[i:i + batch_size]
            batch_embeddings = self.embeddings_model.encode(batch, convert_to_tensor=False)
            embeddings.extend(batch_embeddings)
        
        self.embeddings = np.array(embeddings).astype('float32')
        logger.info(f"‚úÖ Created embeddings: {self.embeddings.shape}")
        
        return True
    
    def build_faiss_index(self):
        """Build FAISS vector index"""
        
        if not FAISS_AVAILABLE:
            logger.error("‚ùå FAISS not available")
            return False
        
        if len(self.embeddings) == 0:
            logger.error("‚ùå No embeddings to index")
            return False
        
        logger.info("üîç Building FAISS index...")
        
        # Create FAISS index
        dimension = self.embeddings.shape[1]
        index = faiss.IndexFlatIP(dimension)  # Inner product (cosine similarity)
        
        # Normalize embeddings for cosine similarity
        faiss.normalize_L2(self.embeddings)
        
        # Add embeddings to index
        index.add(self.embeddings)
        
        # Save FAISS index
        index_file = self.output_dir / "whis_cybersecurity_knowledge.faiss"
        faiss.write_index(index, str(index_file))
        
        logger.info(f"‚úÖ FAISS index saved: {index_file} ({index.ntotal} vectors)")
        
        # Save chunk metadata
        metadata_file = self.output_dir / "whis_cybersecurity_knowledge.metadata.json"
        with open(metadata_file, 'w') as f:
            json.dump({
                "chunks": self.chunks,
                "knowledge_base": self.knowledge_base,
                "index_info": {
                    "dimension": dimension,
                    "total_vectors": int(index.ntotal),
                    "index_type": "IndexFlatIP",
                    "normalized": True
                }
            }, f, indent=2, default=str)
        
        logger.info(f"‚úÖ Metadata saved: {metadata_file}")
        
        # Update knowledge base stats
        self.knowledge_base["chunk_count"] = len(self.chunks)
        self.knowledge_base["embedding_dimension"] = dimension
        
        return True
    
    def update_pointers(self):
        """Update pointer files for production use"""
        
        pointers_file = Path("ai-training/rag/indices/pointers.json")
        pointers_file.parent.mkdir(parents=True, exist_ok=True)
        
        pointers = {
            "current_index": "whis_cybersecurity_knowledge",
            "indices": {
                "whis_cybersecurity_knowledge": {
                    "faiss_file": "ai-training/rag/vectorstore/whis_cybersecurity_knowledge.faiss",
                    "metadata_file": "ai-training/rag/vectorstore/whis_cybersecurity_knowledge.metadata.json",
                    "created_at": datetime.utcnow().isoformat(),
                    "version": "1.0.0",
                    "chunk_count": len(self.chunks),
                    "description": "Comprehensive cybersecurity knowledge base"
                }
            }
        }
        
        with open(pointers_file, 'w') as f:
            json.dump(pointers, f, indent=2)
        
        logger.info(f"‚úÖ Updated pointers: {pointers_file}")
    
    def run_full_pipeline(self):
        """Run the complete knowledge base creation pipeline"""
        
        logger.info("üöÄ Starting comprehensive cybersecurity knowledge base creation...")
        
        # Check dependencies
        missing_deps = []
        if not HF_AVAILABLE:
            missing_deps.append("datasets")
        if not ST_AVAILABLE:
            missing_deps.append("sentence-transformers")  
        if not FAISS_AVAILABLE:
            missing_deps.append("faiss-cpu")
        
        if missing_deps:
            logger.error(f"‚ùå Missing dependencies: {missing_deps}")
            logger.info("Install with: pip install " + " ".join(missing_deps))
            return False
        
        # Step 1: Create base glossary
        logger.info("üìö Step 1: Creating base cybersecurity glossary...")
        glossary_count = self.create_base_glossary()
        
        # Step 2: Add system configuration
        logger.info("‚öôÔ∏è  Step 2: Adding WHIS system configuration...")
        config_count = self.add_local_security_content()
        
        # Step 3: Ingest HuggingFace datasets
        logger.info("üì• Step 3: Ingesting Hugging Face cybersecurity datasets...")
        hf_count = self.ingest_huggingface_datasets()
        
        # Step 3b: Ingest Open-MalSec threat intelligence
        logger.info("üîç Step 3b: Ingesting Open-MalSec threat intelligence...")
        malsec_count = self.ingest_open_malsec_data()
        
        # Step 3c: Ingest forensics data
        logger.info("üïµÔ∏è Step 3c: Ingesting forensics and SIEM data...")
        forensics_count = self.ingest_forensics_data()
        
        # Step 3d: Ingest SOAR datasets
        logger.info("üöÄ Step 3d: Ingesting elite SOAR datasets...")
        soar_count = self.ingest_soar_datasets()
        
        total_chunks = len(self.chunks)
        logger.info(f"üìä Total knowledge chunks: {total_chunks}")
        logger.info(f"  - Glossary: {glossary_count}")
        logger.info(f"  - System config: {config_count}") 
        logger.info(f"  - HuggingFace datasets: {hf_count}")
        logger.info(f"  - Open-MalSec data: {malsec_count}")
        logger.info(f"  - Forensics data: {forensics_count}")
        logger.info(f"  - SOAR datasets: {soar_count}")
        
        if total_chunks == 0:
            logger.error("‚ùå No content to embed")
            return False
        
        # Step 4: Create embeddings
        logger.info("üßÆ Step 4: Creating embeddings...")
        if not self.create_embeddings():
            return False
        
        # Step 5: Build FAISS index
        logger.info("üîç Step 5: Building FAISS vector index...")
        if not self.build_faiss_index():
            return False
        
        # Step 6: Update pointers
        logger.info("üîó Step 6: Updating index pointers...")
        self.update_pointers()
        
        logger.info("üéâ Knowledge base creation completed successfully!")
        logger.info(f"üìç FAISS index: {self.output_dir}/whis_cybersecurity_knowledge.faiss")
        logger.info(f"üìç Metadata: {self.output_dir}/whis_cybersecurity_knowledge.metadata.json")
        logger.info(f"üìç Chunks: {self.chunks_dir}")
        
        return True

def main():
    """Main execution function"""
    print("üß† WHIS Cybersecurity Knowledge Base Builder")
    print("=" * 50)
    
    builder = CybersecurityKnowledgeBuilder()
    success = builder.run_full_pipeline()
    
    if success:
        print("\n‚úÖ Knowledge base ready for RAG integration!")
        print("üîó Run this server with RAG_ENABLED=true to use the new knowledge base")
    else:
        print("\n‚ùå Knowledge base creation failed")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())