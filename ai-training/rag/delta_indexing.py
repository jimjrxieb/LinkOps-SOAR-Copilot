#!/usr/bin/env python3
"""
üîÑ Delta Indexing Pipeline
=========================
Session delta ingestion - only new/changed chunks, not full rebuilds.

Senior-level delta processing:
- Session outcome capture
- Incremental upserts only
- Smoke RAG eval before promotion
- Versioned index pointers with rollback
- Git diff-based change detection
"""

import os
import json
import hashlib
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import git
from dataclasses import dataclass, asdict

# Add parent directories
import sys
sys.path.append(str(Path(__file__).parent.parent))

from rag.hybrid_retrieval import HybridRetriever, RetrievalResult
from rag.repo_ingestion import RepoIngestionPipeline, CodeChunk
from eval.run_eval import WhisEvaluator


@dataclass
class SessionDelta:
    """Session outcome capture for indexing"""
    session_id: str
    timestamp: datetime
    session_type: str  # conversation, training, edit, deployment
    changes_summary: str
    decisions: List[str]
    new_facts: List[str]
    todos: List[str]
    files_modified: List[str]
    git_commit: Optional[str]
    user_context: Optional[str]
    
    def to_indexable_content(self) -> str:
        """Convert session to indexable text"""
        content_parts = [
            f"Session: {self.session_type} on {self.timestamp.strftime('%Y-%m-%d %H:%M')}",
            "",
            "Summary:",
            self.changes_summary,
            ""
        ]
        
        if self.decisions:
            content_parts.extend([
                "Key Decisions:",
                *[f"- {decision}" for decision in self.decisions],
                ""
            ])
            
        if self.new_facts:
            content_parts.extend([
                "New Facts/Learning:",
                *[f"- {fact}" for fact in self.new_facts],
                ""
            ])
            
        if self.todos:
            content_parts.extend([
                "Action Items:",
                *[f"- {todo}" for todo in self.todos],
                ""
            ])
            
        if self.files_modified:
            content_parts.extend([
                "Files Modified:",
                *[f"- {file}" for file in self.files_modified],
                ""
            ])
            
        return "\n".join(content_parts)


class DeltaIndexingPipeline:
    """Delta-only indexing for sessions and code changes"""
    
    def __init__(self, config_path: str = "ai-training/configs/delta_indexing.yaml"):
        self.config_path = Path(config_path)
        self.config = self._load_config()
        
        # Initialize components
        self.retriever = HybridRetriever()
        self.repo_ingestion = RepoIngestionPipeline(".")
        
        # State tracking
        self.state_file = Path("ai-training/rag/.delta_state.json")
        self.state = self._load_state()
        
    def _load_config(self) -> Dict[str, Any]:
        """Load delta indexing configuration"""
        if not self.config_path.exists():
            default_config = {
                "session_capture": {
                    "enabled": True,
                    "auto_capture_threshold_minutes": 5,  # Auto-capture sessions > 5 min
                    "max_summary_length": 1000,
                    "include_code_context": True
                },
                "delta_detection": {
                    "git_enabled": True,
                    "track_file_patterns": [
                        "**/*.py", "**/*.js", "**/*.ts", "**/*.go", 
                        "**/*.md", "**/*.yaml", "**/*.json"
                    ],
                    "ignore_patterns": [
                        "**/__pycache__/**", "**/node_modules/**", 
                        "**/.git/**", "**/venv/**"
                    ]
                },
                "indexing": {
                    "chunk_size": 300,
                    "overlap": 50,
                    "batch_size": 20,
                    "collections": {
                        "sessions": "whis_sessions",
                        "code_deltas": "whis_code_deltas",
                        "docs_deltas": "whis_docs_deltas"
                    }
                },
                "evaluation": {
                    "smoke_test_enabled": True,
                    "golden_queries": [
                        "What did we work on recently?",
                        "What code changes were made?",
                        "What decisions were made?",
                        "What are the pending action items?"
                    ],
                    "faithfulness_threshold": 0.70,
                    "relevancy_threshold": 0.75,
                    "max_eval_time_seconds": 30
                },
                "versioning": {
                    "keep_versions": 10,
                    "auto_promote_on_success": True,
                    "rollback_on_failure": True
                }
            }
            
            self.config_path.parent.mkdir(parents=True, exist_ok=True)
            import yaml
            with open(self.config_path, 'w') as f:
                yaml.dump(default_config, f, indent=2)
                
            return default_config
            
        import yaml
        with open(self.config_path, 'r') as f:
            return yaml.safe_load(f)
            
    def _load_state(self) -> Dict[str, Any]:
        """Load delta indexing state"""
        if self.state_file.exists():
            with open(self.state_file, 'r') as f:
                return json.load(f)
        else:
            return {
                "last_git_commit": "",
                "last_session_time": "",
                "index_versions": {},
                "current_pointers": {
                    "live": None,
                    "staging": None
                },
                "session_history": [],
                "delta_stats": {
                    "total_sessions_indexed": 0,
                    "total_deltas_processed": 0,
                    "last_promotion": ""
                }
            }
            
    def _save_state(self):
        """Save delta indexing state"""
        self.state_file.parent.mkdir(parents=True, exist_ok=True)
        with open(self.state_file, 'w') as f:
            json.dump(self.state, f, indent=2, default=str)
            
    def capture_session_delta(self, session_data: Dict[str, Any]) -> SessionDelta:
        """Capture session outcome as delta"""
        print("üìù Capturing session delta...")
        
        # Auto-generate session summary if not provided
        if not session_data.get("changes_summary"):
            session_data["changes_summary"] = self._auto_generate_summary(session_data)
            
        # Get git info
        try:
            repo = git.Repo(".")
            current_commit = repo.head.commit.hexsha
            files_modified = [
                item.a_path for item in repo.index.diff("HEAD") 
                if item.a_path
            ] + list(repo.untracked_files)
        except:
            current_commit = None
            files_modified = []
            
        session_delta = SessionDelta(
            session_id=session_data.get("session_id", self._generate_session_id()),
            timestamp=datetime.now(),
            session_type=session_data.get("session_type", "conversation"),
            changes_summary=session_data.get("changes_summary", ""),
            decisions=session_data.get("decisions", []),
            new_facts=session_data.get("new_facts", []),
            todos=session_data.get("todos", []),
            files_modified=files_modified,
            git_commit=current_commit,
            user_context=session_data.get("user_context", "")
        )
        
        print(f"üìä Session captured: {session_delta.session_type} with {len(files_modified)} file changes")
        return session_delta
        
    def _auto_generate_summary(self, session_data: Dict[str, Any]) -> str:
        """Auto-generate session summary from context"""
        # Simplified auto-summary
        summary_parts = []
        
        if session_data.get("files_modified"):
            summary_parts.append(f"Modified {len(session_data['files_modified'])} files")
            
        if session_data.get("session_type") == "training":
            summary_parts.append("Completed model training session")
        elif session_data.get("session_type") == "conversation":
            summary_parts.append("Interactive conversation session")
        elif session_data.get("session_type") == "deployment":
            summary_parts.append("Deployment and configuration session")
            
        return "; ".join(summary_parts) if summary_parts else "Session completed"
        
    def _generate_session_id(self) -> str:
        """Generate unique session ID"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        random_suffix = hashlib.sha256(str(datetime.now()).encode()).hexdigest()[:6]
        return f"session_{timestamp}_{random_suffix}"
        
    def detect_code_deltas(self, since_commit: Optional[str] = None) -> List[CodeChunk]:
        """Detect code changes since last commit"""
        print("üîç Detecting code deltas...")
        
        if not since_commit:
            since_commit = self.state["last_git_commit"]
            
        if not since_commit:
            print("‚ö†Ô∏è  No previous commit found - skipping delta detection")
            return []
            
        try:
            # Get changed files
            changed_files = self.repo_ingestion.get_changed_files(since_commit)
            
            if not changed_files:
                print("‚úÖ No code changes detected")
                return []
                
            # Process only changed files
            delta_chunks = []
            
            for file_path in changed_files:
                if file_path.suffix in ['.py', '.js', '.ts', '.go', '.java', '.cpp']:
                    chunks = self.repo_ingestion.chunk_code_file(file_path)
                    delta_chunks.extend(chunks)
                elif file_path.suffix in ['.md', '.yaml', '.json', '.txt']:
                    chunks = self.repo_ingestion.chunk_docs_file(file_path)
                    delta_chunks.extend(chunks)
                    
            print(f"üìä Detected {len(delta_chunks)} delta chunks from {len(changed_files)} files")
            return delta_chunks
            
        except Exception as e:
            print(f"‚ö†Ô∏è  Error detecting code deltas: {e}")
            return []
            
    def upsert_session_delta(self, session_delta: SessionDelta) -> Dict[str, Any]:
        """Upsert session delta to index"""
        print(f"üì§ Upserting session delta: {session_delta.session_id}")
        
        try:
            # Get sessions collection
            collection = self.retriever.vector_stores.get("docs")  # Use docs collection for sessions
            if not collection:
                raise RuntimeError("Sessions collection not available")
                
            # Convert to indexable content
            content = session_delta.to_indexable_content()
            
            # Generate embedding
            embeddings = self.retriever.dual_embedder.embed_chunk({
                "content": content,
                "chunk_type": "session",
                "language": "text"
            })
            
            if "prose" not in embeddings:
                raise RuntimeError("Failed to generate session embedding")
                
            # Prepare metadata
            metadata = {
                "session_id": session_delta.session_id,
                "session_type": session_delta.session_type,
                "timestamp": session_delta.timestamp.isoformat(),
                "git_commit": session_delta.git_commit or "",
                "files_modified_count": len(session_delta.files_modified),
                "chunk_type": "session",
                "symbol_type": "session_outcome",
                "file_path": f"sessions/{session_delta.session_id}",
                "symbol_name": f"session_{session_delta.session_type}",
                "start_line": 1,
                "end_line": len(content.split('\n')),
                "language": "text",
                "commit_hash": session_delta.git_commit or "",
                "module_path": "",
                "last_modified": session_delta.timestamp.isoformat()
            }
            
            # Upsert to collection
            collection.upsert(
                ids=[session_delta.session_id],
                documents=[content],
                embeddings=[embeddings["prose"].tolist()],
                metadatas=[metadata]
            )
            
            # Update state
            self.state["session_history"].append({
                "session_id": session_delta.session_id,
                "timestamp": session_delta.timestamp.isoformat(),
                "type": session_delta.session_type,
                "files_count": len(session_delta.files_modified)
            })
            
            self.state["delta_stats"]["total_sessions_indexed"] += 1
            self.state["last_session_time"] = session_delta.timestamp.isoformat()
            
            self._save_state()
            
            return {
                "success": True,
                "session_id": session_delta.session_id,
                "content_length": len(content),
                "metadata": metadata
            }
            
        except Exception as e:
            print(f"‚ùå Failed to upsert session delta: {e}")
            return {"success": False, "error": str(e)}
            
    def upsert_code_deltas(self, delta_chunks: List[CodeChunk]) -> Dict[str, Any]:
        """Upsert code deltas to appropriate collections"""
        print(f"üì§ Upserting {len(delta_chunks)} code deltas...")
        
        if not delta_chunks:
            return {"success": True, "processed": 0}
            
        results = {"code": 0, "docs": 0, "config": 0, "errors": []}
        
        # Group chunks by type
        chunk_groups = {"code": [], "docs": [], "config": []}
        
        for chunk in delta_chunks:
            if chunk.chunk_type == "code":
                chunk_groups["code"].append(chunk)
            elif chunk.chunk_type in ["config", "schema"]:
                chunk_groups["config"].append(chunk)  
            else:
                chunk_groups["docs"].append(chunk)
                
        # Process each group
        for group_type, chunks in chunk_groups.items():
            if not chunks:
                continue
                
            try:
                collection = self.retriever.vector_stores.get(group_type)
                if not collection:
                    continue
                    
                # Prepare batch data
                ids = []
                documents = []
                embeddings = []
                metadatas = []
                
                for chunk in chunks:
                    # Generate unique ID
                    chunk_id = hashlib.sha256(
                        f"{chunk.file_path}:{chunk.start_line}:{chunk.end_line}:{chunk.commit_hash}".encode()
                    ).hexdigest()[:16]
                    
                    # Get appropriate embedding
                    chunk_dict = {
                        "content": chunk.content,
                        "chunk_type": chunk.chunk_type,
                        "language": chunk.language
                    }
                    chunk_embeddings = self.retriever.dual_embedder.embed_chunk(chunk_dict)
                    
                    if group_type == "code" and "code" in chunk_embeddings:
                        embedding = chunk_embeddings["code"]
                    elif "prose" in chunk_embeddings:
                        embedding = chunk_embeddings["prose"]
                    else:
                        continue
                        
                    # Prepare metadata
                    metadata = {
                        "file_path": chunk.file_path,
                        "symbol_name": chunk.symbol_name,
                        "symbol_type": chunk.symbol_type,
                        "start_line": chunk.start_line,
                        "end_line": chunk.end_line,
                        "language": chunk.language,
                        "commit_hash": chunk.commit_hash,
                        "module_path": chunk.module_path,
                        "chunk_type": chunk.chunk_type,
                        "last_modified": chunk.last_modified.isoformat(),
                        "indexed_at": datetime.now().isoformat()
                    }
                    
                    ids.append(chunk_id)
                    documents.append(chunk.content)
                    embeddings.append(embedding.tolist())
                    metadatas.append(metadata)
                    
                # Batch upsert
                if ids:
                    collection.upsert(
                        ids=ids,
                        documents=documents,
                        embeddings=embeddings,
                        metadatas=metadatas
                    )
                    
                    results[group_type] = len(ids)
                    
            except Exception as e:
                error_msg = f"Failed to upsert {group_type} deltas: {e}"
                print(f"‚ö†Ô∏è  {error_msg}")
                results["errors"].append(error_msg)
                
        # Update state
        total_processed = sum(results[k] for k in ["code", "docs", "config"])
        self.state["delta_stats"]["total_deltas_processed"] += total_processed
        
        # Update git commit tracking
        if delta_chunks:
            self.state["last_git_commit"] = delta_chunks[0].commit_hash
            
        self._save_state()
        
        return {
            "success": len(results["errors"]) == 0,
            "processed": total_processed,
            "breakdown": {k: v for k, v in results.items() if k != "errors"},
            "errors": results["errors"]
        }
        
    def run_smoke_eval(self) -> Dict[str, Any]:
        """Run smoke RAG evaluation on golden queries"""
        print("üß™ Running smoke RAG evaluation...")
        
        if not self.config["evaluation"]["smoke_test_enabled"]:
            return {"passed": True, "reason": "smoke_test_disabled"}
            
        golden_queries = self.config["evaluation"]["golden_queries"]
        
        try:
            results = []
            
            for query in golden_queries:
                # Perform retrieval
                retrieved = self.retriever.hybrid_search(query, top_k=3)
                
                if not retrieved:
                    results.append({
                        "query": query,
                        "retrieved_count": 0,
                        "max_score": 0.0,
                        "has_citation": False
                    })
                    continue
                    
                # Check retrieval quality
                max_score = max(r.final_score for r in retrieved)
                has_citation = any(r.file_path for r in retrieved)
                
                results.append({
                    "query": query,
                    "retrieved_count": len(retrieved),
                    "max_score": max_score,
                    "has_citation": has_citation,
                    "top_result_type": retrieved[0].chunk_type if retrieved else None
                })
                
            # Evaluate results
            relevancy_scores = [r["max_score"] for r in results if r["max_score"] > 0]
            avg_relevancy = sum(relevancy_scores) / len(relevancy_scores) if relevancy_scores else 0
            
            citation_rate = sum(1 for r in results if r["has_citation"]) / len(results)
            retrieval_rate = sum(1 for r in results if r["retrieved_count"] > 0) / len(results)
            
            # Check thresholds
            relevancy_threshold = self.config["evaluation"]["relevancy_threshold"]
            passed = (
                avg_relevancy >= relevancy_threshold and
                citation_rate >= 0.8 and  # 80% should have citations
                retrieval_rate >= 0.9     # 90% should retrieve something
            )
            
            eval_result = {
                "passed": passed,
                "metrics": {
                    "avg_relevancy": avg_relevancy,
                    "citation_rate": citation_rate,
                    "retrieval_rate": retrieval_rate,
                    "queries_evaluated": len(results)
                },
                "thresholds": {
                    "relevancy_threshold": relevancy_threshold,
                    "citation_threshold": 0.8,
                    "retrieval_threshold": 0.9
                },
                "details": results,
                "timestamp": datetime.now().isoformat()
            }
            
            print(f"üìä Smoke eval: {'‚úÖ PASSED' if passed else '‚ùå FAILED'}")
            print(f"    Relevancy: {avg_relevancy:.3f} (>= {relevancy_threshold})")
            print(f"    Citations: {citation_rate:.1%} (>= 80%)")
            print(f"    Retrieval: {retrieval_rate:.1%} (>= 90%)")
            
            return eval_result
            
        except Exception as e:
            print(f"‚ùå Smoke evaluation failed: {e}")
            return {"passed": False, "error": str(e)}
            
    def create_index_version(self) -> str:
        """Create new index version after successful delta processing"""
        version = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        version_metadata = {
            "version": version,
            "created_at": datetime.now().isoformat(),
            "git_commit": self.state["last_git_commit"],
            "session_count": self.state["delta_stats"]["total_sessions_indexed"],
            "delta_count": self.state["delta_stats"]["total_deltas_processed"],
            "status": "staging"
        }
        
        self.state["index_versions"][version] = version_metadata
        self.state["current_pointers"]["staging"] = version
        
        self._save_state()
        
        print(f"üì¶ Created index version: {version}")
        return version
        
    def promote_to_live(self, version: str) -> bool:
        """Promote staging version to live"""
        if version not in self.state["index_versions"]:
            print(f"‚ùå Version {version} not found")
            return False
            
        # Promote (simplified - in practice would update routing)
        self.state["current_pointers"]["live"] = version
        self.state["index_versions"][version]["status"] = "live"
        self.state["index_versions"][version]["promoted_at"] = datetime.now().isoformat()
        
        self.state["delta_stats"]["last_promotion"] = datetime.now().isoformat()
        
        self._save_state()
        
        print(f"üöÄ Promoted version {version} to live")
        return True
        
    def process_session_delta(self, session_data: Dict[str, Any]) -> Dict[str, Any]:
        """Complete session delta processing pipeline"""
        print("üîÑ Processing session delta...")
        
        try:
            # Capture session
            session_delta = self.capture_session_delta(session_data)
            
            # Detect code changes
            code_deltas = self.detect_code_deltas()
            
            # Upsert session
            session_result = self.upsert_session_delta(session_delta)
            if not session_result["success"]:
                return {"success": False, "error": session_result["error"]}
                
            # Upsert code deltas
            delta_result = self.upsert_code_deltas(code_deltas)
            if not delta_result["success"]:
                return {"success": False, "error": "Code delta upsert failed"}
                
            # Run smoke evaluation
            eval_result = self.run_smoke_eval()
            
            # Create new version
            if eval_result["passed"]:
                version = self.create_index_version()
                
                # Auto-promote if configured
                if self.config["versioning"]["auto_promote_on_success"]:
                    self.promote_to_live(version)
                    
                pipeline_result = {
                    "success": True,
                    "session_id": session_delta.session_id,
                    "session_result": session_result,
                    "delta_result": delta_result,
                    "eval_result": eval_result,
                    "version": version,
                    "promoted": self.config["versioning"]["auto_promote_on_success"],
                    "timestamp": datetime.now().isoformat()
                }
            else:
                # Evaluation failed
                pipeline_result = {
                    "success": False,
                    "error": "Smoke evaluation failed",
                    "session_id": session_delta.session_id,
                    "eval_result": eval_result,
                    "timestamp": datetime.now().isoformat()
                }
                
                # Rollback if configured
                if self.config["versioning"]["rollback_on_failure"]:
                    print("üîÑ Rolling back due to failed evaluation")
                    # Would implement rollback logic
                    
            print(f"{'‚úÖ' if pipeline_result['success'] else '‚ùå'} Session delta processing complete")
            return pipeline_result
            
        except Exception as e:
            print(f"‚ùå Session delta processing failed: {e}")
            return {"success": False, "error": str(e)}


def main():
    """CLI for delta indexing"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Delta indexing pipeline")
    parser.add_argument("--config", default="ai-training/configs/delta_indexing.yaml")
    parser.add_argument("--session-data", help="JSON file with session data")
    parser.add_argument("--session-type", default="conversation", help="Session type")
    parser.add_argument("--changes-summary", help="Session changes summary")
    parser.add_argument("--smoke-eval-only", action="store_true", help="Run smoke eval only")
    
    args = parser.parse_args()
    
    # Initialize pipeline
    pipeline = DeltaIndexingPipeline(args.config)
    
    if args.smoke_eval_only:
        # Run smoke evaluation only
        result = pipeline.run_smoke_eval()
        print(json.dumps(result, indent=2))
        sys.exit(0 if result["passed"] else 1)
        
    # Process session delta
    if args.session_data:
        with open(args.session_data, 'r') as f:
            session_data = json.load(f)
    else:
        session_data = {
            "session_type": args.session_type,
            "changes_summary": args.changes_summary or "Session completed",
            "decisions": [],
            "new_facts": [],
            "todos": []
        }
        
    result = pipeline.process_session_delta(session_data)
    
    print("\nüìä Final Result:")
    print(json.dumps(result, indent=2, default=str))
    
    sys.exit(0 if result["success"] else 1)


if __name__ == "__main__":
    main()