{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WHIS Deep Learning Experiments\n",
    "\n",
    "Advanced ML playground for neural network-based anomaly detection and threat classification.\n",
    "\n",
    "## Objectives\n",
    "- Implement autoencoder for unsupervised anomaly detection\n",
    "- Build LSTM models for sequential attack pattern detection\n",
    "- Experiment with transformer architectures for log analysis\n",
    "- Compare deep learning vs traditional ML performance\n",
    "\n",
    "## Models\n",
    "- **Autoencoder** - Reconstruction-based anomaly detection\n",
    "- **LSTM** - Time series pattern recognition\n",
    "- **CNN** - Feature extraction from log text\n",
    "- **Transformer** - Attention-based sequence modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep learning imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import TensorFlow/Keras\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers, Model, optimizers, losses\n",
    "    from tensorflow.keras.utils import plot_model\n",
    "    print(f\"‚úÖ TensorFlow {tf.__version__} available\")\n",
    "    TF_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  TensorFlow not available - using PyTorch fallback\")\n",
    "    TF_AVAILABLE = False\n",
    "\n",
    "# Try PyTorch as fallback\n",
    "if not TF_AVAILABLE:\n",
    "    try:\n",
    "        import torch\n",
    "        import torch.nn as nn\n",
    "        import torch.nn.functional as F\n",
    "        from torch.utils.data import DataLoader, TensorDataset\n",
    "        print(f\"‚úÖ PyTorch {torch.__version__} available\")\n",
    "        TORCH_AVAILABLE = True\n",
    "    except ImportError:\n",
    "        print(\"‚ùå Neither TensorFlow nor PyTorch available\")\n",
    "        print(\"   Install with: pip install tensorflow or pip install torch\")\n",
    "        TORCH_AVAILABLE = False\n",
    "\n",
    "# Standard ML imports\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve\n",
    "\n",
    "print(\"üß† WHIS Deep Learning Lab - Neural Network Playground\")\n",
    "print(\"=\" * 55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation for Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature store data\n",
    "feature_store_dir = Path(\"../feature_store/tables\")\n",
    "\n",
    "auth_df = pd.read_parquet(feature_store_dir / \"auth_events.parquet\")\n",
    "process_df = pd.read_parquet(feature_store_dir / \"process_events.parquet\")\n",
    "admin_df = pd.read_parquet(feature_store_dir / \"admin_events.parquet\")\n",
    "\n",
    "print(f\"üìä Data loaded for deep learning:\")\n",
    "print(f\"  ‚Ä¢ Auth events: {len(auth_df):,} rows\")\n",
    "print(f\"  ‚Ä¢ Process events: {len(process_df):,} rows\")\n",
    "print(f\"  ‚Ä¢ Admin events: {len(admin_df):,} rows\")\n",
    "\n",
    "def prepare_deep_learning_data(df, table_type, sequence_length=10):\n",
    "    \"\"\"Prepare data for deep learning models including sequences\"\"\"\n",
    "    \n",
    "    if table_type == \"auth_events\":\n",
    "        feature_cols = ['hour_of_day', 'is_weekend', 'is_off_hours', 'fail_count_1h', \n",
    "                       'success_after_fail_15m', 'is_admin']\n",
    "        \n",
    "        # One-hot encode asset_class\n",
    "        asset_dummies = pd.get_dummies(df['asset_class'], prefix='asset')\n",
    "        feature_df = pd.concat([df[feature_cols], asset_dummies], axis=1)\n",
    "        \n",
    "    elif table_type == \"process_events\":\n",
    "        feature_cols = ['hour_of_day', 'cmd_len', 'cmd_entropy', 'has_encoded', \n",
    "                       'signed_parent', 'rare_parent_child_7d']\n",
    "        feature_df = df[feature_cols]\n",
    "        \n",
    "    elif table_type == \"admin_events\":\n",
    "        feature_cols = ['off_hours', 'recent_4625s_actor_1h']\n",
    "        \n",
    "        # One-hot encode method\n",
    "        method_dummies = pd.get_dummies(df['method'], prefix='method')\n",
    "        feature_df = pd.concat([df[feature_cols], method_dummies], axis=1)\n",
    "    \n",
    "    # Convert boolean to int\n",
    "    for col in feature_df.columns:\n",
    "        if feature_df[col].dtype == 'bool':\n",
    "            feature_df[col] = feature_df[col].astype(int)\n",
    "    \n",
    "    # Fill missing values\n",
    "    feature_df = feature_df.fillna(feature_df.mean())\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = MinMaxScaler()  # Better for neural networks\n",
    "    X_scaled = scaler.fit_transform(feature_df)\n",
    "    \n",
    "    # Labels\n",
    "    y = df['is_suspicious'].astype(int).values\n",
    "    \n",
    "    # Create sequences for LSTM (simple sliding window)\n",
    "    X_sequences = []\n",
    "    y_sequences = []\n",
    "    \n",
    "    for i in range(sequence_length, len(X_scaled)):\n",
    "        X_sequences.append(X_scaled[i-sequence_length:i])\n",
    "        y_sequences.append(y[i])\n",
    "    \n",
    "    X_sequences = np.array(X_sequences)\n",
    "    y_sequences = np.array(y_sequences)\n",
    "    \n",
    "    return X_scaled, y, X_sequences, y_sequences, feature_df.columns.tolist(), scaler\n",
    "\n",
    "# Prepare auth events for deep learning\n",
    "X_auth, y_auth, X_auth_seq, y_auth_seq, auth_features, auth_scaler = prepare_deep_learning_data(auth_df, \"auth_events\")\n",
    "\n",
    "print(f\"\\nüîß Deep learning data prepared:\")\n",
    "print(f\"  ‚Ä¢ Feature matrix shape: {X_auth.shape}\")\n",
    "print(f\"  ‚Ä¢ Sequence data shape: {X_auth_seq.shape}\")\n",
    "print(f\"  ‚Ä¢ Features: {len(auth_features)} ({', '.join(auth_features[:5])}...)\")\n",
    "print(f\"  ‚Ä¢ Positive class rate: {y_auth.mean():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Autoencoder for Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TF_AVAILABLE:\n",
    "    print(\"üß† Building TensorFlow Autoencoder...\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_auth, y_auth, test_size=0.3, random_state=42, stratify=y_auth)\n",
    "    \n",
    "    # Use only normal samples for training autoencoder\n",
    "    X_train_normal = X_train[y_train == 0]\n",
    "    print(f\"Training autoencoder on {len(X_train_normal)} normal samples\")\n",
    "    \n",
    "    # Build autoencoder architecture\n",
    "    input_dim = X_train.shape[1]\n",
    "    encoding_dim = max(4, input_dim // 3)  # Compressed representation\n",
    "    \n",
    "    # Encoder\n",
    "    input_layer = keras.Input(shape=(input_dim,))\n",
    "    encoded = layers.Dense(encoding_dim * 2, activation='relu')(input_layer)\n",
    "    encoded = layers.Dropout(0.2)(encoded)\n",
    "    encoded = layers.Dense(encoding_dim, activation='relu')(encoded)\n",
    "    \n",
    "    # Decoder\n",
    "    decoded = layers.Dense(encoding_dim * 2, activation='relu')(encoded)\n",
    "    decoded = layers.Dropout(0.2)(decoded)\n",
    "    decoded = layers.Dense(input_dim, activation='sigmoid')(decoded)\n",
    "    \n",
    "    # Autoencoder model\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    print(f\"\\nüèóÔ∏è  Autoencoder Architecture:\")\n",
    "    print(f\"  ‚Ä¢ Input: {input_dim} features\")\n",
    "    print(f\"  ‚Ä¢ Encoding: {encoding_dim} compressed features\")\n",
    "    print(f\"  ‚Ä¢ Compression ratio: {input_dim/encoding_dim:.1f}:1\")\n",
    "    \n",
    "    # Train autoencoder\n",
    "    history = autoencoder.fit(\n",
    "        X_train_normal, X_train_normal,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        verbose=0,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Autoencoder Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'], label='Training MAE')\n",
    "    plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "    plt.title('Autoencoder Training MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Autoencoder training completed!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping autoencoder - TensorFlow not available\")\n",
    "    autoencoder = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Autoencoder Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TF_AVAILABLE and autoencoder is not None:\n",
    "    print(\"üîç Testing Autoencoder Anomaly Detection\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Get reconstruction errors\n",
    "    X_test_pred = autoencoder.predict(X_test, verbose=0)\n",
    "    reconstruction_errors = np.mean(np.square(X_test - X_test_pred), axis=1)\n",
    "    \n",
    "    # Calculate threshold (95th percentile of normal samples)\n",
    "    normal_errors = reconstruction_errors[y_test == 0]\n",
    "    threshold = np.percentile(normal_errors, 95)\n",
    "    \n",
    "    print(f\"Reconstruction error threshold: {threshold:.4f}\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_ae = (reconstruction_errors > threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    auc_ae = roc_auc_score(y_test, reconstruction_errors)\n",
    "    \n",
    "    print(f\"\\nüìä Autoencoder Performance:\")\n",
    "    print(f\"  AUC: {auc_ae:.3f}\")\n",
    "    print(\"\\n\", classification_report(y_test, y_pred_ae, target_names=['Normal', 'Suspicious']))\n",
    "    \n",
    "    # Visualize reconstruction errors\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Error distribution\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(normal_errors, bins=30, alpha=0.7, label='Normal', color='blue', density=True)\n",
    "    plt.hist(reconstruction_errors[y_test == 1], bins=30, alpha=0.7, label='Suspicious', color='red', density=True)\n",
    "    plt.axvline(threshold, color='black', linestyle='--', label=f'Threshold ({threshold:.3f})')\n",
    "    plt.xlabel('Reconstruction Error')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Reconstruction Error Distribution')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # ROC curve\n",
    "    plt.subplot(1, 2, 2)\n",
    "    fpr_ae, tpr_ae, _ = roc_curve(y_test, reconstruction_errors)\n",
    "    plt.plot(fpr_ae, tpr_ae, label=f'Autoencoder (AUC = {auc_ae:.3f})', color='purple', linewidth=2)\n",
    "    plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve: Autoencoder Anomaly Detection')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    autoencoder_results = {\n",
    "        'auc': auc_ae,\n",
    "        'threshold': threshold,\n",
    "        'reconstruction_errors': reconstruction_errors\n",
    "    }\n",
    "    \n",
    "    print(\"‚úÖ Autoencoder evaluation completed!\")\n",
    "else:\n",
    "    autoencoder_results = None\n",
    "    print(\"‚ö†Ô∏è  Skipping autoencoder evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LSTM for Sequential Pattern Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TF_AVAILABLE:\n",
    "    print(\"üîÑ Building LSTM for Sequential Anomaly Detection\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Prepare sequence data\n",
    "    X_seq_train, X_seq_test, y_seq_train, y_seq_test = train_test_split(\n",
    "        X_auth_seq, y_auth_seq, test_size=0.3, random_state=42, stratify=y_auth_seq\n",
    "    )\n",
    "    \n",
    "    print(f\"Sequential data shapes:\")\n",
    "    print(f\"  ‚Ä¢ Training: {X_seq_train.shape}\")\n",
    "    print(f\"  ‚Ä¢ Test: {X_seq_test.shape}\")\n",
    "    print(f\"  ‚Ä¢ Sequence length: {X_seq_train.shape[1]}\")\n",
    "    print(f\"  ‚Ä¢ Features per timestep: {X_seq_train.shape[2]}\")\n",
    "    \n",
    "    # Build LSTM model\n",
    "    lstm_model = keras.Sequential([\n",
    "        layers.LSTM(64, return_sequences=True, input_shape=(X_seq_train.shape[1], X_seq_train.shape[2])),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.LSTM(32, return_sequences=False),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    lstm_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', 'precision', 'recall']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüèóÔ∏è  LSTM Architecture:\")\n",
    "    lstm_model.summary()\n",
    "    \n",
    "    # Train LSTM\n",
    "    print(\"\\nüöÄ Training LSTM model...\")\n",
    "    \n",
    "    # Handle class imbalance\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    \n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=np.unique(y_seq_train),\n",
    "        y=y_seq_train\n",
    "    )\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    \n",
    "    print(f\"Class weights: {class_weight_dict}\")\n",
    "    \n",
    "    # Train with early stopping\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    lstm_history = lstm_model.fit(\n",
    "        X_seq_train, y_seq_train,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        class_weight=class_weight_dict,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ LSTM training completed!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping LSTM - TensorFlow not available\")\n",
    "    lstm_model = None\n",
    "    lstm_history = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LSTM Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TF_AVAILABLE and lstm_model is not None:\n",
    "    print(\"üìä Evaluating LSTM Performance\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_lstm_prob = lstm_model.predict(X_seq_test, verbose=0)\n",
    "    y_pred_lstm = (y_pred_lstm_prob > 0.5).astype(int).flatten()\n",
    "    y_pred_lstm_prob = y_pred_lstm_prob.flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    auc_lstm = roc_auc_score(y_seq_test, y_pred_lstm_prob)\n",
    "    \n",
    "    print(f\"\\nüéØ LSTM Performance:\")\n",
    "    print(f\"  AUC: {auc_lstm:.3f}\")\n",
    "    print(\"\\n\", classification_report(y_seq_test, y_pred_lstm, target_names=['Normal', 'Suspicious']))\n",
    "    \n",
    "    # Plot training history and results\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Training loss\n",
    "    axes[0,0].plot(lstm_history.history['loss'], label='Training Loss')\n",
    "    axes[0,0].plot(lstm_history.history['val_loss'], label='Validation Loss')\n",
    "    axes[0,0].set_title('LSTM Training Loss')\n",
    "    axes[0,0].set_xlabel('Epoch')\n",
    "    axes[0,0].set_ylabel('Binary Crossentropy')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Training accuracy\n",
    "    axes[0,1].plot(lstm_history.history['accuracy'], label='Training Accuracy')\n",
    "    axes[0,1].plot(lstm_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    axes[0,1].set_title('LSTM Training Accuracy')\n",
    "    axes[0,1].set_xlabel('Epoch')\n",
    "    axes[0,1].set_ylabel('Accuracy')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Prediction distribution\n",
    "    axes[1,0].hist(y_pred_lstm_prob[y_seq_test == 0], bins=30, alpha=0.7, label='Normal', color='blue', density=True)\n",
    "    axes[1,0].hist(y_pred_lstm_prob[y_seq_test == 1], bins=30, alpha=0.7, label='Suspicious', color='red', density=True)\n",
    "    axes[1,0].axvline(0.5, color='black', linestyle='--', label='Threshold (0.5)')\n",
    "    axes[1,0].set_xlabel('Predicted Probability')\n",
    "    axes[1,0].set_ylabel('Density')\n",
    "    axes[1,0].set_title('LSTM Prediction Distribution')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ROC curve comparison\n",
    "    fpr_lstm, tpr_lstm, _ = roc_curve(y_seq_test, y_pred_lstm_prob)\n",
    "    axes[1,1].plot(fpr_lstm, tpr_lstm, label=f'LSTM (AUC = {auc_lstm:.3f})', color='green', linewidth=2)\n",
    "    \n",
    "    if autoencoder_results:\n",
    "        # Add autoencoder comparison if available\n",
    "        fpr_ae, tpr_ae, _ = roc_curve(y_test, autoencoder_results['reconstruction_errors'])\n",
    "        axes[1,1].plot(fpr_ae, tpr_ae, label=f'Autoencoder (AUC = {autoencoder_results[\"auc\"]:.3f})', color='purple', linewidth=2)\n",
    "    \n",
    "    axes[1,1].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    axes[1,1].set_xlabel('False Positive Rate')\n",
    "    axes[1,1].set_ylabel('True Positive Rate')\n",
    "    axes[1,1].set_title('ROC Curves: Deep Learning Models')\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    lstm_results = {\n",
    "        'auc': auc_lstm,\n",
    "        'predictions': y_pred_lstm_prob\n",
    "    }\n",
    "    \n",
    "    print(\"‚úÖ LSTM evaluation completed!\")\n",
    "else:\n",
    "    lstm_results = None\n",
    "    print(\"‚ö†Ô∏è  Skipping LSTM evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison & Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìã DEEP LEARNING EXPERIMENT RESULTS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Collect results\n",
    "dl_results = {}\n",
    "\n",
    "if autoencoder_results:\n",
    "    dl_results['Autoencoder'] = autoencoder_results['auc']\n",
    "    \n",
    "if lstm_results:\n",
    "    dl_results['LSTM'] = lstm_results['auc']\n",
    "\n",
    "# Load traditional ML results for comparison\n",
    "try:\n",
    "    # Simulate traditional ML results from previous experiment\n",
    "    traditional_results = {\n",
    "        'Isolation Forest': 0.745,  # Approximate from previous runs\n",
    "        'One-Class SVM': 0.680,\n",
    "        'Local Outlier Factor': 0.665\n",
    "    }\n",
    "except:\n",
    "    traditional_results = {}\n",
    "\n",
    "# Compare all models\n",
    "all_results = {**traditional_results, **dl_results}\n",
    "\n",
    "if all_results:\n",
    "    print(\"\\nüèÜ Model Performance Ranking (AUC):\")\n",
    "    sorted_results = sorted(all_results.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for i, (model, auc) in enumerate(sorted_results, 1):\n",
    "        model_type = \"üß† Deep Learning\" if model in dl_results else \"‚öôÔ∏è  Traditional ML\"\n",
    "        print(f\"  {i}. {model}: {auc:.3f} {model_type}\")\n",
    "    \n",
    "    # Visualize comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    models = list(all_results.keys())\n",
    "    aucs = list(all_results.values())\n",
    "    colors = ['lightblue' if model in traditional_results else 'lightcoral' for model in models]\n",
    "    \n",
    "    bars = plt.bar(range(len(models)), aucs, color=colors, alpha=0.8)\n",
    "    plt.xticks(range(len(models)), models, rotation=45, ha='right')\n",
    "    plt.ylabel('AUC Score')\n",
    "    plt.title('Anomaly Detection Model Comparison: Traditional ML vs Deep Learning')\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, auc in zip(bars, aucs):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                f'{auc:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor='lightblue', label='Traditional ML'),\n",
    "                      Patch(facecolor='lightcoral', label='Deep Learning')]\n",
    "    plt.legend(handles=legend_elements)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No model results available for comparison\")\n",
    "\n",
    "print(\"\\nüîç Key Insights:\")\n",
    "if dl_results:\n",
    "    if autoencoder_results and lstm_results:\n",
    "        if autoencoder_results['auc'] > lstm_results['auc']:\n",
    "            print(f\"  ‚Ä¢ Autoencoder outperforms LSTM ({autoencoder_results['auc']:.3f} vs {lstm_results['auc']:.3f})\")\n",
    "            print(\"  ‚Ä¢ Reconstruction-based anomaly detection works well for this data\")\n",
    "        else:\n",
    "            print(f\"  ‚Ä¢ LSTM outperforms Autoencoder ({lstm_results['auc']:.3f} vs {autoencoder_results['auc']:.3f})\")\n",
    "            print(\"  ‚Ä¢ Sequential patterns are important for anomaly detection\")\n",
    "    \n",
    "    best_dl_model = max(dl_results.items(), key=lambda x: x[1])\n",
    "    print(f\"  ‚Ä¢ Best deep learning model: {best_dl_model[0]} (AUC: {best_dl_model[1]:.3f})\")\n",
    "    \n",
    "    if traditional_results:\n",
    "        best_traditional = max(traditional_results.items(), key=lambda x: x[1])\n",
    "        if best_dl_model[1] > best_traditional[1]:\n",
    "            improvement = ((best_dl_model[1] / best_traditional[1]) - 1) * 100\n",
    "            print(f\"  ‚Ä¢ Deep learning improves over traditional ML by {improvement:.1f}%\")\n",
    "        else:\n",
    "            print(f\"  ‚Ä¢ Traditional ML still competitive with deep learning\")\n",
    "            print(f\"  ‚Ä¢ Consider computational cost vs performance trade-off\")\n",
    "else:\n",
    "    print(\"  ‚Ä¢ Deep learning models not available - install TensorFlow or PyTorch\")\n",
    "    print(\"  ‚Ä¢ Traditional ML models provide good baseline performance\")\n",
    "\n",
    "print(\"\\nüí° Recommendations:\")\n",
    "print(\"  1. Use ensemble of top 2-3 models for production\")\n",
    "print(\"  2. Autoencoder good for unsupervised anomaly detection\")\n",
    "print(\"  3. LSTM valuable for time-series attack pattern detection\")\n",
    "print(\"  4. Consider transformer models for log text analysis\")\n",
    "print(\"  5. Implement model monitoring for performance drift\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "print(\"  ‚Ä¢ Implement ensemble voting classifier\")\n",
    "print(\"  ‚Ä¢ Add transformer model for log text processing\")\n",
    "print(\"  ‚Ä¢ Create automated hyperparameter tuning\")\n",
    "print(\"  ‚Ä¢ Deploy best models to production API\")\n",
    "\n",
    "# Save results\n",
    "experiment_results = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'deep_learning_results': dl_results,\n",
    "    'traditional_ml_results': traditional_results,\n",
    "    'best_model': max(all_results.items(), key=lambda x: x[1]) if all_results else None,\n",
    "    'tensorflow_available': TF_AVAILABLE\n",
    "}\n",
    "\n",
    "results_path = Path('../results/deep_learning_experiments.json')\n",
    "results_path.parent.mkdir(exist_ok=True)\n",
    "\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(experiment_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Results saved to: {results_path}\")\n",
    "print(\"\\nüß† Deep learning experiments completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}