{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WHIS Model Promotion Pipeline\n",
    "\n",
    "Automated pipeline for promoting experimental models to production with validation and deployment.\n",
    "\n",
    "## Objectives\n",
    "- Validate experimental models against production criteria\n",
    "- Automated model testing and performance benchmarking\n",
    "- Safe deployment with rollback capabilities\n",
    "- Model monitoring and drift detection setup\n",
    "\n",
    "## Pipeline Stages\n",
    "1. **Model Validation** - Performance, stability, security checks\n",
    "2. **Integration Testing** - API compatibility, load testing\n",
    "3. **Staging Deployment** - Shadow mode testing\n",
    "4. **Production Promotion** - Blue-green deployment\n",
    "5. **Monitoring Setup** - Performance tracking, alerts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import shutil\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML imports\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# WHIS imports\n",
    "import sys\n",
    "sys.path.append('../models')\n",
    "from isolation_forest_anomaly import WhisAnomalyDetector\n",
    "\n",
    "print(\"🚀 WHIS Model Promotion Pipeline\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'model_artifacts_dir': Path('../models/artifacts'),\n",
    "    'staging_dir': Path('../models/staging'),\n",
    "    'production_dir': Path('../models/production'),\n",
    "    'validation_data_dir': Path('../feature_store/tables'),\n",
    "    'api_endpoint': 'http://localhost:8000',\n",
    "    'performance_thresholds': {\n",
    "        'min_auc': 0.70,\n",
    "        'min_precision': 0.60,\n",
    "        'min_recall': 0.50,\n",
    "        'max_inference_time_ms': 100,\n",
    "        'max_model_size_mb': 50\n",
    "    },\n",
    "    'deployment_config': {\n",
    "        'shadow_mode_duration_hours': 24,\n",
    "        'rollback_threshold_error_rate': 0.05,\n",
    "        'monitoring_check_interval_minutes': 5\n",
    "    }\n",
    "}\n",
    "\n",
    "# Ensure directories exist\n",
    "for dir_path in [CONFIG['staging_dir'], CONFIG['production_dir']]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"📁 Pipeline directories configured\")\n",
    "print(f\"  • Artifacts: {CONFIG['model_artifacts_dir']}\")\n",
    "print(f\"  • Staging: {CONFIG['staging_dir']}\")\n",
    "print(f\"  • Production: {CONFIG['production_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Discovery & Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_candidate_models():\n",
    "    \"\"\"Find trained models available for promotion\"\"\"\n",
    "    \n",
    "    artifacts_dir = CONFIG['model_artifacts_dir']\n",
    "    candidates = []\n",
    "    \n",
    "    if artifacts_dir.exists():\n",
    "        for model_dir in artifacts_dir.iterdir():\n",
    "            if model_dir.is_dir() and (model_dir / 'metadata.json').exists():\n",
    "                # Load metadata\n",
    "                with open(model_dir / 'metadata.json', 'r') as f:\n",
    "                    metadata = json.load(f)\n",
    "                \n",
    "                # Check if required files exist\n",
    "                required_files = ['models.joblib', 'scalers.joblib', 'encoders.joblib']\n",
    "                if all((model_dir / f).exists() for f in required_files):\n",
    "                    candidates.append({\n",
    "                        'path': model_dir,\n",
    "                        'name': metadata.get('model_name', model_dir.name),\n",
    "                        'created_at': metadata.get('created_at'),\n",
    "                        'version': metadata.get('version', '1.0'),\n",
    "                        'model_types': metadata.get('model_types', []),\n",
    "                        'metadata': metadata\n",
    "                    })\n",
    "    \n",
    "    # Sort by creation date (newest first)\n",
    "    candidates.sort(key=lambda x: x['created_at'], reverse=True)\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "# Discover available models\n",
    "candidate_models = discover_candidate_models()\n",
    "\n",
    "print(f\"🔍 Discovered {len(candidate_models)} candidate models:\")\n",
    "if candidate_models:\n",
    "    for i, model in enumerate(candidate_models, 1):\n",
    "        print(f\"  {i}. {model['name']} (v{model['version']})\")\n",
    "        print(f\"     Created: {model['created_at']}\")\n",
    "        print(f\"     Types: {', '.join(model['model_types'])}\")\n",
    "        print(f\"     Path: {model['path']}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"  No trained models found!\")\n",
    "    print(\"  Run anomaly detection training first.\")\n",
    "\n",
    "# Select most recent model for promotion\n",
    "if candidate_models:\n",
    "    selected_model = candidate_models[0]\n",
    "    print(f\"🎯 Selected for promotion: {selected_model['name']}\")\n",
    "else:\n",
    "    selected_model = None\n",
    "    print(\"❌ No models available for promotion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model_performance(model_path, validation_data_dir):\n",
    "    \"\"\"Comprehensive model validation against production criteria\"\"\"\n",
    "    \n",
    "    print(f\"🔬 Validating model: {model_path}\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    validation_results = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'model_path': str(model_path),\n",
    "        'passed': False,\n",
    "        'tests': {},\n",
    "        'performance_metrics': {},\n",
    "        'issues': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Load the model\n",
    "        detector = WhisAnomalyDetector.load_model(model_path)\n",
    "        \n",
    "        # Load validation data\n",
    "        auth_df = pd.read_parquet(validation_data_dir / \"auth_events.parquet\")\n",
    "        process_df = pd.read_parquet(validation_data_dir / \"process_events.parquet\")\n",
    "        admin_df = pd.read_parquet(validation_data_dir / \"admin_events.parquet\")\n",
    "        \n",
    "        datasets = {\n",
    "            'auth_events': auth_df,\n",
    "            'process_events': process_df,\n",
    "            'admin_events': admin_df\n",
    "        }\n",
    "        \n",
    "        print(\"\\n📊 Performance Validation:\")\n",
    "        \n",
    "        all_tests_passed = True\n",
    "        \n",
    "        for table_type, df in datasets.items():\n",
    "            if table_type not in detector.models:\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\n🧪 Testing {table_type}...\")\n",
    "            \n",
    "            # Prepare features\n",
    "            X = detector.prepare_features(df, table_type)\n",
    "            y = df['is_suspicious'] if 'is_suspicious' in df.columns else None\n",
    "            \n",
    "            if y is None:\n",
    "                print(f\"   ⚠️  No labels available for {table_type}\")\n",
    "                continue\n",
    "            \n",
    "            # Performance testing\n",
    "            start_time = time.time()\n",
    "            anomaly_scores = detector.predict_anomaly_score(df, table_type)\n",
    "            inference_time = (time.time() - start_time) * 1000  # ms\n",
    "            \n",
    "            # Calculate metrics\n",
    "            auc = roc_auc_score(y, anomaly_scores) if len(np.unique(y)) > 1 else 0.5\n",
    "            \n",
    "            # Binary predictions for precision/recall\n",
    "            threshold = np.percentile(anomaly_scores, 85)  # Top 15% as anomalies\n",
    "            y_pred = (anomaly_scores > threshold).astype(int)\n",
    "            \n",
    "            report = classification_report(y, y_pred, output_dict=True, zero_division=0)\n",
    "            precision = report.get('1', {}).get('precision', 0.0)\n",
    "            recall = report.get('1', {}).get('recall', 0.0)\n",
    "            \n",
    "            # Store metrics\n",
    "            validation_results['performance_metrics'][table_type] = {\n",
    "                'auc': auc,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'inference_time_ms': inference_time,\n",
    "                'samples_tested': len(df)\n",
    "            }\n",
    "            \n",
    "            # Check thresholds\n",
    "            thresholds = CONFIG['performance_thresholds']\n",
    "            \n",
    "            tests = {\n",
    "                'auc_test': auc >= thresholds['min_auc'],\n",
    "                'precision_test': precision >= thresholds['min_precision'],\n",
    "                'recall_test': recall >= thresholds['min_recall'],\n",
    "                'inference_time_test': inference_time <= thresholds['max_inference_time_ms']\n",
    "            }\n",
    "            \n",
    "            validation_results['tests'][table_type] = tests\n",
    "            \n",
    "            # Report results\n",
    "            print(f\"   AUC: {auc:.3f} {'✅' if tests['auc_test'] else '❌'} (min: {thresholds['min_auc']})\")\n",
    "            print(f\"   Precision: {precision:.3f} {'✅' if tests['precision_test'] else '❌'} (min: {thresholds['min_precision']})\")\n",
    "            print(f\"   Recall: {recall:.3f} {'✅' if tests['recall_test'] else '❌'} (min: {thresholds['min_recall']})\")\n",
    "            print(f\"   Inference: {inference_time:.1f}ms {'✅' if tests['inference_time_test'] else '❌'} (max: {thresholds['max_inference_time_ms']}ms)\")\n",
    "            \n",
    "            # Check if all tests passed for this table\n",
    "            if not all(tests.values()):\n",
    "                all_tests_passed = False\n",
    "                failed_tests = [test for test, passed in tests.items() if not passed]\n",
    "                validation_results['issues'].append(f\"{table_type}: Failed tests - {', '.join(failed_tests)}\")\n",
    "        \n",
    "        # Model size check\n",
    "        model_size_mb = sum(f.stat().st_size for f in model_path.rglob('*') if f.is_file()) / (1024 * 1024)\n",
    "        size_test_passed = model_size_mb <= CONFIG['performance_thresholds']['max_model_size_mb']\n",
    "        \n",
    "        print(f\"\\n📦 Model Size: {model_size_mb:.1f}MB {'✅' if size_test_passed else '❌'} (max: {CONFIG['performance_thresholds']['max_model_size_mb']}MB)\")\n",
    "        \n",
    "        if not size_test_passed:\n",
    "            all_tests_passed = False\n",
    "            validation_results['issues'].append(f\"Model size too large: {model_size_mb:.1f}MB\")\n",
    "        \n",
    "        validation_results['passed'] = all_tests_passed\n",
    "        \n",
    "        if all_tests_passed:\n",
    "            print(\"\\n✅ All validation tests PASSED!\")\n",
    "        else:\n",
    "            print(\"\\n❌ Validation FAILED with issues:\")\n",
    "            for issue in validation_results['issues']:\n",
    "                print(f\"   • {issue}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        validation_results['issues'].append(f\"Validation error: {str(e)}\")\n",
    "        print(f\"\\n💥 Validation failed with error: {e}\")\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# Run validation if we have a selected model\n",
    "if selected_model:\n",
    "    validation_results = validate_model_performance(\n",
    "        selected_model['path'], \n",
    "        CONFIG['validation_data_dir']\n",
    "    )\n",
    "    \n",
    "    # Save validation report\n",
    "    report_path = CONFIG['staging_dir'] / f\"validation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    with open(report_path, 'w') as f:\n",
    "        json.dump(validation_results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n💾 Validation report saved: {report_path}\")\n",
    "else:\n",
    "    validation_results = None\n",
    "    print(\"⚠️  Skipping validation - no model selected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Staging Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_to_staging(model_path, validation_results):\n",
    "    \"\"\"Deploy validated model to staging environment\"\"\"\n",
    "    \n",
    "    print(\"🏗️  Deploying to staging environment...\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    if not validation_results or not validation_results.get('passed', False):\n",
    "        print(\"❌ Cannot deploy - model failed validation\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Create staging deployment\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        staging_path = CONFIG['staging_dir'] / f\"whis_anomaly_detector_staging_{timestamp}\"\n",
    "        \n",
    "        # Copy model artifacts\n",
    "        shutil.copytree(model_path, staging_path)\n",
    "        \n",
    "        # Create staging metadata\n",
    "        staging_metadata = {\n",
    "            'deployment_type': 'staging',\n",
    "            'deployed_at': datetime.now().isoformat(),\n",
    "            'source_model': str(model_path),\n",
    "            'validation_passed': True,\n",
    "            'validation_report': validation_results,\n",
    "            'staging_path': str(staging_path),\n",
    "            'status': 'active',\n",
    "            'shadow_mode': True,\n",
    "            'performance_monitoring': {\n",
    "                'start_time': datetime.now().isoformat(),\n",
    "                'end_time': (datetime.now() + timedelta(hours=CONFIG['deployment_config']['shadow_mode_duration_hours'])).isoformat(),\n",
    "                'metrics_collected': [],\n",
    "                'issues_detected': []\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save staging metadata\n",
    "        with open(staging_path / 'staging_metadata.json', 'w') as f:\n",
    "            json.dump(staging_metadata, f, indent=2)\n",
    "        \n",
    "        # Create symlink to latest staging\n",
    "        latest_staging = CONFIG['staging_dir'] / 'latest'\n",
    "        if latest_staging.exists():\n",
    "            latest_staging.unlink()\n",
    "        latest_staging.symlink_to(staging_path.name)\n",
    "        \n",
    "        print(f\"✅ Staging deployment successful!\")\n",
    "        print(f\"   Path: {staging_path}\")\n",
    "        print(f\"   Shadow mode duration: {CONFIG['deployment_config']['shadow_mode_duration_hours']} hours\")\n",
    "        \n",
    "        return {\n",
    "            'staging_path': staging_path,\n",
    "            'metadata': staging_metadata,\n",
    "            'deployment_time': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"💥 Staging deployment failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Deploy to staging if validation passed\n",
    "if validation_results and validation_results.get('passed', False):\n",
    "    staging_deployment = deploy_to_staging(selected_model['path'], validation_results)\n",
    "else:\n",
    "    staging_deployment = None\n",
    "    if validation_results:\n",
    "        print(\"⚠️  Skipping staging deployment - validation failed\")\n",
    "    else:\n",
    "        print(\"⚠️  Skipping staging deployment - no validation results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Integration Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_integration_tests(staging_deployment):\n",
    "    \"\"\"Run integration tests against staging deployment\"\"\"\n",
    "    \n",
    "    print(\"🧪 Running integration tests...\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    if not staging_deployment:\n",
    "        print(\"❌ No staging deployment available\")\n",
    "        return None\n",
    "    \n",
    "    integration_results = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'staging_path': str(staging_deployment['staging_path']),\n",
    "        'tests': {},\n",
    "        'passed': False,\n",
    "        'issues': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Test 1: Model Loading\n",
    "        print(\"\\n🔧 Test 1: Model Loading\")\n",
    "        try:\n",
    "            detector = WhisAnomalyDetector.load_model(staging_deployment['staging_path'])\n",
    "            integration_results['tests']['model_loading'] = {'passed': True, 'message': 'Model loaded successfully'}\n",
    "            print(\"   ✅ Model loads correctly\")\n",
    "        except Exception as e:\n",
    "            integration_results['tests']['model_loading'] = {'passed': False, 'message': f'Model loading failed: {e}'}\n",
    "            integration_results['issues'].append(f'Model loading: {e}')\n",
    "            print(f\"   ❌ Model loading failed: {e}\")\n",
    "            return integration_results\n",
    "        \n",
    "        # Test 2: Prediction API\n",
    "        print(\"\\n🔧 Test 2: Prediction Functionality\")\n",
    "        try:\n",
    "            # Create sample data for each model type\n",
    "            test_data = {\n",
    "                'auth_events': pd.DataFrame({\n",
    "                    'hour_of_day': [3, 10],\n",
    "                    'is_weekend': [True, False],\n",
    "                    'is_off_hours': [True, False],\n",
    "                    'fail_count_1h': [5, 0],\n",
    "                    'success_after_fail_15m': [True, False],\n",
    "                    'is_admin': [True, False],\n",
    "                    'asset_class': ['server', 'workstation']\n",
    "                }),\n",
    "                'process_events': pd.DataFrame({\n",
    "                    'hour_of_day': [2, 14],\n",
    "                    'cmd_len': [150, 50],\n",
    "                    'cmd_entropy': [4.5, 2.0],\n",
    "                    'has_encoded': [True, False],\n",
    "                    'signed_parent': [False, True],\n",
    "                    'rare_parent_child_7d': [True, False]\n",
    "                }),\n",
    "                'admin_events': pd.DataFrame({\n",
    "                    'off_hours': [True, False],\n",
    "                    'recent_4625s_actor_1h': [3, 0],\n",
    "                    'method': ['registry', 'group']\n",
    "                })\n",
    "            }\n",
    "            \n",
    "            prediction_tests_passed = True\n",
    "            \n",
    "            for table_type, sample_df in test_data.items():\n",
    "                if table_type in detector.models:\n",
    "                    scores = detector.predict_anomaly_score(sample_df, table_type)\n",
    "                    \n",
    "                    # Validate scores\n",
    "                    if len(scores) == len(sample_df) and all(0 <= score <= 1 for score in scores):\n",
    "                        print(f\"   ✅ {table_type}: Predictions valid (scores: {scores.round(3)})\")\n",
    "                    else:\n",
    "                        prediction_tests_passed = False\n",
    "                        integration_results['issues'].append(f'{table_type}: Invalid prediction scores')\n",
    "                        print(f\"   ❌ {table_type}: Invalid prediction scores\")\n",
    "            \n",
    "            integration_results['tests']['prediction_api'] = {\n",
    "                'passed': prediction_tests_passed,\n",
    "                'message': 'All prediction tests passed' if prediction_tests_passed else 'Some prediction tests failed'\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            integration_results['tests']['prediction_api'] = {'passed': False, 'message': f'Prediction test failed: {e}'}\n",
    "            integration_results['issues'].append(f'Prediction API: {e}')\n",
    "            print(f\"   ❌ Prediction test failed: {e}\")\n",
    "        \n",
    "        # Test 3: Performance Stress Test\n",
    "        print(\"\\n🔧 Test 3: Performance Stress Test\")\n",
    "        try:\n",
    "            # Generate larger test dataset\n",
    "            stress_data = pd.DataFrame({\n",
    "                'hour_of_day': np.random.randint(0, 24, 1000),\n",
    "                'is_weekend': np.random.choice([True, False], 1000),\n",
    "                'is_off_hours': np.random.choice([True, False], 1000),\n",
    "                'fail_count_1h': np.random.randint(0, 10, 1000),\n",
    "                'success_after_fail_15m': np.random.choice([True, False], 1000),\n",
    "                'is_admin': np.random.choice([True, False], 1000),\n",
    "                'asset_class': np.random.choice(['server', 'workstation', 'mobile'], 1000)\n",
    "            })\n",
    "            \n",
    "            # Time the prediction\n",
    "            start_time = time.time()\n",
    "            stress_scores = detector.predict_anomaly_score(stress_data, 'auth_events')\n",
    "            stress_time = (time.time() - start_time) * 1000  # ms\n",
    "            \n",
    "            throughput = len(stress_data) / (stress_time / 1000)  # samples/second\n",
    "            \n",
    "            stress_test_passed = stress_time <= CONFIG['performance_thresholds']['max_inference_time_ms'] * 10  # 10x buffer for stress\n",
    "            \n",
    "            integration_results['tests']['stress_test'] = {\n",
    "                'passed': stress_test_passed,\n",
    "                'message': f'Processed {len(stress_data)} samples in {stress_time:.1f}ms ({throughput:.1f} samples/sec)',\n",
    "                'throughput_samples_per_sec': throughput\n",
    "            }\n",
    "            \n",
    "            if stress_test_passed:\n",
    "                print(f\"   ✅ Stress test passed: {throughput:.1f} samples/sec\")\n",
    "            else:\n",
    "                integration_results['issues'].append(f'Stress test failed: {stress_time:.1f}ms too slow')\n",
    "                print(f\"   ❌ Stress test failed: {stress_time:.1f}ms too slow\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            integration_results['tests']['stress_test'] = {'passed': False, 'message': f'Stress test failed: {e}'}\n",
    "            integration_results['issues'].append(f'Stress test: {e}')\n",
    "            print(f\"   ❌ Stress test failed: {e}\")\n",
    "        \n",
    "        # Overall result\n",
    "        all_tests_passed = all(test.get('passed', False) for test in integration_results['tests'].values())\n",
    "        integration_results['passed'] = all_tests_passed\n",
    "        \n",
    "        if all_tests_passed:\n",
    "            print(\"\\n✅ All integration tests PASSED!\")\n",
    "        else:\n",
    "            print(\"\\n❌ Some integration tests FAILED:\")\n",
    "            for issue in integration_results['issues']:\n",
    "                print(f\"   • {issue}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        integration_results['issues'].append(f'Integration test error: {e}')\n",
    "        print(f\"\\n💥 Integration testing failed: {e}\")\n",
    "    \n",
    "    return integration_results\n",
    "\n",
    "# Run integration tests\n",
    "if staging_deployment:\n",
    "    integration_results = run_integration_tests(staging_deployment)\n",
    "    \n",
    "    # Save integration test report\n",
    "    if integration_results:\n",
    "        report_path = CONFIG['staging_dir'] / f\"integration_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        with open(report_path, 'w') as f:\n",
    "            json.dump(integration_results, f, indent=2)\n",
    "        print(f\"\\n💾 Integration test report saved: {report_path}\")\nelse:\n    integration_results = None\n    print(\"⚠️  Skipping integration tests - no staging deployment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Production Promotion Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_promotion_decision(validation_results, integration_results):\n",
    "    \"\"\"Automated decision for production promotion\"\"\"\n",
    "    \n",
    "    print(\"🎯 Making production promotion decision...\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    decision = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'promote_to_production': False,\n",
    "        'reasons': [],\n",
    "        'blockers': [],\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    # Check validation results\n",
    "    if not validation_results or not validation_results.get('passed', False):\n",
    "        decision['blockers'].append('Model failed performance validation')\n",
    "        if validation_results and 'issues' in validation_results:\n",
    "            decision['blockers'].extend(validation_results['issues'])\n",
    "    else:\n",
    "        decision['reasons'].append('Model passed all validation tests')\n",
    "        \n",
    "        # Highlight strong performance\n",
    "        for table_type, metrics in validation_results.get('performance_metrics', {}).items():\n",
    "            auc = metrics.get('auc', 0)\n",
    "            if auc >= 0.85:\n",
    "                decision['reasons'].append(f'{table_type} shows excellent performance (AUC: {auc:.3f})')\n",
    "    \n",
    "    # Check integration results\n",
    "    if not integration_results or not integration_results.get('passed', False):\n",
    "        decision['blockers'].append('Model failed integration tests')\n",
    "        if integration_results and 'issues' in integration_results:\n",
    "            decision['blockers'].extend(integration_results['issues'])\n",
    "    else:\n",
    "        decision['reasons'].append('Model passed all integration tests')\n",
    "        \n",
    "        # Check throughput\n",
    "        stress_test = integration_results.get('tests', {}).get('stress_test', {})\n",
    "        if stress_test.get('passed', False):\n",
    "            throughput = stress_test.get('throughput_samples_per_sec', 0)\n",
    "            decision['reasons'].append(f'Good performance throughput: {throughput:.1f} samples/sec')\n",
    "    \n",
    "    # Make final decision\n",
    "    if not decision['blockers']:\n",
    "        decision['promote_to_production'] = True\n",
    "        decision['recommendations'] = [\n",
    "            'Deploy with blue-green strategy for safe rollback',\n",
    "            'Monitor performance metrics for first 24 hours',\n",
    "            'Set up automated drift detection alerts',\n",
    "            'Plan regular model retraining schedule'\n",
    "        ]\n",
    "    else:\n",
    "        decision['recommendations'] = [\n",
    "            'Fix identified issues before promotion',\n",
    "            'Re-run validation and integration tests',\n",
    "            'Consider model architecture improvements',\n",
    "            'Review training data quality'\n",
    "        ]\n",
    "    \n",
    "    # Report decision\n",
    "    if decision['promote_to_production']:\n",
    "        print(\"\\n🎉 PROMOTION APPROVED!\")\n",
    "        print(\"\\n✅ Reasons for approval:\")\n",
    "        for reason in decision['reasons']:\n",
    "            print(f\"   • {reason}\")\n",
    "    else:\n",
    "        print(\"\\n🚫 PROMOTION BLOCKED!\")\n",
    "        print(\"\\n❌ Blocking issues:\")\n",
    "        for blocker in decision['blockers']:\n",
    "            print(f\"   • {blocker}\")\n",
    "    \n",
    "    print(\"\\n💡 Recommendations:\")\n",
    "    for rec in decision['recommendations']:\n",
    "        print(f\"   • {rec}\")\n",
    "    \n",
    "    return decision\n",
    "\n",
    "# Make promotion decision\n",
    "if validation_results or integration_results:\n",
    "    promotion_decision = make_promotion_decision(validation_results, integration_results)\n",
    "    \n",
    "    # Save decision report\n",
    "    decision_path = CONFIG['staging_dir'] / f\"promotion_decision_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    with open(decision_path, 'w') as f:\n",
    "        json.dump(promotion_decision, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n💾 Promotion decision saved: {decision_path}\")\nelse:\n    promotion_decision = None\n    print(\"⚠️  Cannot make promotion decision - no test results available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Production Deployment (If Approved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_to_production(staging_deployment, promotion_decision):\n",
    "    \"\"\"Deploy approved model to production with blue-green strategy\"\"\"\n",
    "    \n",
    "    if not promotion_decision or not promotion_decision.get('promote_to_production', False):\n",
    "        print(\"⚠️  Skipping production deployment - not approved\")\n",
    "        return None\n",
    "    \n",
    "    print(\"🚀 Deploying to PRODUCTION environment...\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Backup current production model (if exists)\n",
    "        current_production = CONFIG['production_dir'] / 'current'\n",
    "        if current_production.exists():\n",
    "            backup_path = CONFIG['production_dir'] / f\"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "            shutil.move(current_production, backup_path)\n",
    "            print(f\"📦 Backed up current production to: {backup_path}\")\n",
    "        \n",
    "        # Deploy new model\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        production_path = CONFIG['production_dir'] / f\"whis_anomaly_detector_prod_{timestamp}\"\n",
    "        \n",
    "        # Copy from staging\n",
    "        shutil.copytree(staging_deployment['staging_path'], production_path)\n",
    "        \n",
    "        # Create production metadata\n",
    "        production_metadata = {\n",
    "            'deployment_type': 'production',\n",
    "            'deployed_at': datetime.now().isoformat(),\n",
    "            'source_staging': str(staging_deployment['staging_path']),\n",
    "            'validation_passed': True,\n",
    "            'integration_passed': True,\n",
    "            'promotion_approved': True,\n",
    "            'version': timestamp,\n",
    "            'status': 'active',\n",
    "            'blue_green_deployment': {\n",
    "                'deployment_strategy': 'blue_green',\n",
    "                'cutover_time': datetime.now().isoformat(),\n",
    "                'rollback_available': True,\n",
    "                'monitoring_active': True\n",
    "            },\n",
    "            'monitoring': {\n",
    "                'drift_detection': True,\n",
    "                'performance_tracking': True,\n",
    "                'error_rate_monitoring': True,\n",
    "                'alert_thresholds': {\n",
    "                    'max_error_rate': 0.05,\n",
    "                    'min_throughput_samples_per_sec': 100,\n",
    "                    'max_inference_time_ms': 50\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save production metadata\n",
    "        with open(production_path / 'production_metadata.json', 'w') as f:\n",
    "            json.dump(production_metadata, f, indent=2)\n",
    "        \n",
    "        # Create symlink to current production\n",
    "        current_production.symlink_to(production_path.name)\n",
    "        \n",
    "        # Create deployment manifest\n",
    "        manifest = {\n",
    "            'deployment_id': f\"whis-anomaly-{timestamp}\",\n",
    "            'version': timestamp,\n",
    "            'model_path': str(production_path),\n",
    "            'deployment_time': datetime.now().isoformat(),\n",
    "            'status': 'deployed',\n",
    "            'health_check_url': f\"{CONFIG['api_endpoint']}/health\",\n",
    "            'rollback_command': f\"ln -sfn backup_{timestamp} {current_production}\"\n",
    "        }\n",
    "        \n",
    "        manifest_path = CONFIG['production_dir'] / 'deployment_manifest.json'\n",
    "        with open(manifest_path, 'w') as f:\n",
    "            json.dump(manifest, f, indent=2)\n",
    "        \n",
    "        print(\"✅ Production deployment successful!\")\n",
    "        print(f\"   Path: {production_path}\")\n",
    "        print(f\"   Version: {timestamp}\")\n",
    "        print(f\"   Blue-Green: Enabled with rollback capability\")\n",
    "        print(f\"   Monitoring: Active with drift detection\")\n",
    "        \n",
    "        return {\n",
    "            'production_path': production_path,\n",
    "            'metadata': production_metadata,\n",
    "            'manifest': manifest,\n",
    "            'deployment_time': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"💥 Production deployment failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Deploy to production if approved\n",
    "if staging_deployment and promotion_decision and promotion_decision.get('promote_to_production', False):\n",
    "    production_deployment = deploy_to_production(staging_deployment, promotion_decision)\n",
    "    \n",
    "    if production_deployment:\n",
    "        print(\"\\n🎊 MODEL SUCCESSFULLY PROMOTED TO PRODUCTION!\")\n",
    "        print(\"\\n📋 Next Steps:\")\n",
    "        print(\"   1. Restart WHIS API to load new production model\")\n",
    "        print(\"   2. Monitor performance metrics for 24 hours\")\n",
    "        print(\"   3. Set up automated alerts for model drift\")\n",
    "        print(\"   4. Plan next model training cycle\")\n",
    "        \n",
    "        # Show API restart command\n",
    "        print(\"\\n🔄 API Restart Commands:\")\n",
    "        print(\"   # Stop current API\")\n",
    "        print(\"   pkill -f whis_production_api.py\")\n",
    "        print(\"   # Start with new production model\")\n",
    "        print(\"   python whis_production_api.py\")\nelse:\n    production_deployment = None\n    if promotion_decision and not promotion_decision.get('promote_to_production', False):\n        print(\"\\n🚫 Production deployment skipped - promotion not approved\")\n        print(\"\\n📋 Required Actions:\")\n        for blocker in promotion_decision.get('blockers', []):\n            print(f\"   • Fix: {blocker}\")\n        print(\"\\n🔄 Re-run this pipeline after addressing issues\")\n    else:\n        print(\"⚠️  Skipping production deployment - no promotion decision available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pipeline Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📊 MODEL PROMOTION PIPELINE SUMMARY\")\nprint(\"=\" * 45)\n\n# Collect all pipeline results\npipeline_summary = {\n    'pipeline_run_id': f\"promotion_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n    'timestamp': datetime.now().isoformat(),\n    'stages': {\n        'model_discovery': {\n            'completed': selected_model is not None,\n            'selected_model': selected_model['name'] if selected_model else None\n        },\n        'validation': {\n            'completed': validation_results is not None,\n            'passed': validation_results.get('passed', False) if validation_results else False\n        },\n        'staging_deployment': {\n            'completed': staging_deployment is not None,\n            'path': str(staging_deployment['staging_path']) if staging_deployment else None\n        },\n        'integration_testing': {\n            'completed': integration_results is not None,\n            'passed': integration_results.get('passed', False) if integration_results else False\n        },\n        'promotion_decision': {\n            'completed': promotion_decision is not None,\n            'approved': promotion_decision.get('promote_to_production', False) if promotion_decision else False\n        },\n        'production_deployment': {\n            'completed': production_deployment is not None,\n            'path': str(production_deployment['production_path']) if production_deployment else None\n        }\n    },\n    'overall_success': production_deployment is not None\n}\n\n# Display stage results\nprint(\"\\n🔄 Pipeline Stages:\")\nfor stage_name, stage_info in pipeline_summary['stages'].items():\n    status = \"✅\" if stage_info.get('completed', False) else \"❌\"\n    print(f\"   {status} {stage_name.replace('_', ' ').title()}\")\n    \n    if stage_name == 'validation' and stage_info['completed']:\n        passed = \"✅ PASSED\" if stage_info['passed'] else \"❌ FAILED\"\n        print(f\"      Performance validation: {passed}\")\n    \n    elif stage_name == 'integration_testing' and stage_info['completed']:\n        passed = \"✅ PASSED\" if stage_info['passed'] else \"❌ FAILED\"\n        print(f\"      Integration tests: {passed}\")\n    \n    elif stage_name == 'promotion_decision' and stage_info['completed']:\n        approved = \"✅ APPROVED\" if stage_info['approved'] else \"❌ BLOCKED\"\n        print(f\"      Production promotion: {approved}\")\n\n# Overall result\nif pipeline_summary['overall_success']:\n    print(\"\\n🎉 PIPELINE SUCCESS!\")\n    print(f\"   Model successfully promoted to production\")\n    print(f\"   Production path: {production_deployment['production_path']}\")\n    print(f\"   Ready for API integration\")\nelse:\n    print(\"\\n⚠️  PIPELINE INCOMPLETE\")\n    print(f\"   Pipeline stopped at failed validation or integration tests\")\n    print(f\"   Review issues and re-run pipeline\")\n\n# Performance summary (if available)\nif validation_results and validation_results.get('performance_metrics'):\n    print(\"\\n📈 Model Performance Summary:\")\n    for table_type, metrics in validation_results['performance_metrics'].items():\n        auc = metrics.get('auc', 0)\n        precision = metrics.get('precision', 0)\n        recall = metrics.get('recall', 0)\n        inference_ms = metrics.get('inference_time_ms', 0)\n        \n        print(f\"   • {table_type}:\")\n        print(f\"     AUC: {auc:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}\")\n        print(f\"     Inference: {inference_ms:.1f}ms\")\n\n# Save complete pipeline report\nreport_path = CONFIG['staging_dir'] / f\"pipeline_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\nwith open(report_path, 'w') as f:\n    # Convert Path objects to strings for JSON serialization\n    json_summary = json.loads(json.dumps(pipeline_summary, default=str))\n    json.dump(json_summary, f, indent=2)\n\nprint(f\"\\n💾 Complete pipeline report saved: {report_path}\")\n\nprint(\"\\n🏁 Model Promotion Pipeline Complete!\")\nprint(\"\\n📋 Next Actions:\")\nif pipeline_summary['overall_success']:\n    print(\"   1. Restart WHIS API with new production model\")\n    print(\"   2. Monitor production metrics for 24-48 hours\")\n    print(\"   3. Set up automated model drift monitoring\")\n    print(\"   4. Schedule regular retraining pipeline\")\nelse:\n    print(\"   1. Review validation and integration test failures\")\n    print(\"   2. Improve model or fix identified issues\")\n    print(\"   3. Re-run the promotion pipeline\")\n    print(\"   4. Consider ensemble methods or different algorithms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}