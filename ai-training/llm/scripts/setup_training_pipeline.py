#!/usr/bin/env python3
"""
Whis Cybersecurity LLM Training Pipeline Setup
Complete setup and orchestration of the fine-tuning pipeline
"""

import os
import sys
import json
import asyncio
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Dict, List

# Add current directory to path for imports
sys.path.append(str(Path(__file__).parent))

from model_config import WhisModelConfig, get_model_info
from data_collection import CybersecDataCollector
from evaluation_metrics import CybersecurityEvaluator

class WhisTrainingPipeline:
    """Complete training pipeline orchestrator"""
    
    def __init__(self, project_root: str = None):
        self.project_root = project_root or str(Path(__file__).parent.parent)
        self.training_dir = Path(__file__).parent
        self.config = WhisModelConfig()
        self.setup_directories()
    
    def setup_directories(self):
        """Create necessary directories"""
        directories = [
            "training_data",
            "models",
            "logs",
            "results",
            "checkpoints"
        ]
        
        for directory in directories:
            dir_path = self.training_dir / directory
            dir_path.mkdir(exist_ok=True)
        
        print("üìÅ Training directories created")
    
    def check_dependencies(self) -> bool:
        """Check if all required dependencies are installed"""
        required_packages = [
            "torch", "transformers", "peft", "datasets", 
            "accelerate", "bitsandbytes", "sentence_transformers",
            "pandas", "numpy", "sklearn"
        ]
        
        missing_packages = []
        
        for package in required_packages:
            try:
                __import__(package)
            except ImportError:
                missing_packages.append(package)
        
        if missing_packages:
            print(f"‚ùå Missing packages: {', '.join(missing_packages)}")
            print("Install with: pip install " + " ".join(missing_packages))
            return False
        
        print("‚úÖ All dependencies satisfied")
        return True
    
    def check_gpu_availability(self):
        """Check GPU availability and memory"""
        try:
            import torch
            if torch.cuda.is_available():
                gpu_count = torch.cuda.device_count()
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
                print(f"üöÄ GPU: {gpu_count} device(s), {gpu_memory:.1f}GB memory")
                
                if gpu_memory < 8:
                    print("‚ö†Ô∏è Warning: Low GPU memory. Consider using smaller batch sizes.")
                
                return True
            else:
                print("‚ö†Ô∏è No GPU available. Training will be slow on CPU.")
                return False
        except ImportError:
            print("‚ùå PyTorch not installed")
            return False
    
    async def collect_training_data(self) -> str:
        """Collect and prepare training data"""
        print("üìä Collecting cybersecurity training data...")
        
        async with CybersecDataCollector(
            output_dir=str(self.training_dir / "training_data")
        ) as collector:
            data = await collector.collect_all_data()
        
        # Find the latest data file
        data_files = list((self.training_dir / "training_data").glob("cybersec_training_data_*.json"))
        latest_file = max(data_files, key=lambda x: x.stat().st_mtime)
        
        print(f"‚úÖ Training data collected: {latest_file}")
        return str(latest_file)
    
    def create_training_script(self) -> str:
        """Create executable training script"""
        training_script = f"""#!/usr/bin/env python3
\"\"\"
Whis Cybersecurity LLM Fine-tuning Script
Generated by training pipeline setup
\"\"\"

import os
import json
import torch
from datetime import datetime
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, 
    TrainingArguments, Trainer, DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import Dataset
import sys

# Import configuration
sys.path.append('{self.training_dir}')
from model_config import WhisModelConfig
from evaluation_metrics import CybersecurityEvaluator

def main():
    print("üõ°Ô∏è Starting Whis Cybersecurity LLM Fine-tuning")
    print(f"‚è∞ Start time: {{datetime.now()}}")
    
    # Load configuration
    config = WhisModelConfig()
    
    # Load training data
    with open('{self.training_dir}/training_data/cybersec_training_data_latest.json', 'r') as f:
        training_data = json.load(f)
    
    # Prepare dataset
    formatted_examples = []
    for category, examples in training_data.items():
        for example in examples:
            formatted_text = f\"\"\"Below is an instruction that describes a cybersecurity task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{{example['instruction']}}

### Input:
{{example['input']}}

### Response:
{{example['output']}}\"\"\"
            formatted_examples.append({{'text': formatted_text}})
    
    print(f"üìä Loaded {{len(formatted_examples)}} training examples")
    
    # Initialize model and tokenizer
    print("ü§ó Loading model and tokenizer...")
    
    tokenizer = AutoTokenizer.from_pretrained(config.base_model_id)
    tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        config.base_model_id,
        quantization_config=config.get_bnb_config(),
        device_map="auto",
        torch_dtype=torch.float16
    )
    
    # Prepare for training
    model = prepare_model_for_kbit_training(model)
    model = get_peft_model(model, config.get_lora_config())
    
    # Tokenize data
    def tokenize_function(examples):
        tokens = tokenizer(
            examples['text'],
            truncation=True,
            padding=False,
            max_length=config.max_sequence_length,
            return_overflowing_tokens=False
        )
        tokens["labels"] = tokens["input_ids"].copy()
        return tokens
    
    dataset = Dataset.from_list(formatted_examples)
    train_test_split = dataset.train_test_split(test_size=0.1, seed=42)
    
    train_dataset = train_test_split['train'].map(
        tokenize_function, batched=True,
        remove_columns=train_test_split['train'].column_names
    )
    
    eval_dataset = train_test_split['test'].map(
        tokenize_function, batched=True,
        remove_columns=train_test_split['test'].column_names
    )
    
    # Setup trainer
    training_args = config.get_training_arguments()
    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator
    )
    
    # Train model
    print("üèãÔ∏è Starting training...")
    trainer.train()
    
    # Evaluate model
    print("üìä Evaluating model...")
    eval_results = trainer.evaluate()
    
    # Save model
    print("üíæ Saving model...")
    model.save_pretrained(config.output_dir)
    tokenizer.save_pretrained(config.output_dir)
    
    # Save results
    results = {{
        "training_date": datetime.now().isoformat(),
        "config": config.__dict__,
        "eval_results": eval_results,
        "training_examples": len(formatted_examples)
    }}
    
    with open(f"{{config.output_dir}}/training_results.json", "w") as f:
        json.dump(results, f, indent=2)
    
    print(f"‚úÖ Training complete! Model saved to {{config.output_dir}}")

if __name__ == "__main__":
    main()
"""
        
        script_path = self.training_dir / "train_whis.py"
        with open(script_path, "w") as f:
            f.write(training_script)
        
        # Make executable
        os.chmod(script_path, 0o755)
        
        print(f"üêç Training script created: {script_path}")
        return str(script_path)
    
    def create_jupyter_launcher(self):
        """Create script to launch Jupyter notebook"""
        launcher_script = f"""#!/bin/bash
# Whis Training Jupyter Launcher

echo "üöÄ Starting Whis Cybersecurity LLM Training Environment"
echo "üìÅ Working directory: {self.training_dir}"

# Activate virtual environment if it exists
if [ -d "venv" ]; then
    echo "üêç Activating virtual environment..."
    source venv/bin/activate
fi

# Install requirements if needed
if [ -f "requirements.txt" ]; then
    echo "üì¶ Installing requirements..."
    pip install -r requirements.txt
fi

# Start Jupyter
echo "üìì Starting Jupyter Notebook..."
cd {self.training_dir}
jupyter lab whis_cybersec_finetuning.ipynb --ip=0.0.0.0 --port=8888 --no-browser --allow-root

echo "‚úÖ Jupyter session ended"
"""
        
        launcher_path = self.training_dir / "launch_jupyter.sh"
        with open(launcher_path, "w") as f:
            f.write(launcher_script)
        
        os.chmod(launcher_path, 0o755)
        
        print(f"üìì Jupyter launcher created: {launcher_path}")
    
    def create_requirements_file(self):
        """Create requirements.txt for training environment"""
        requirements = [
            "torch>=2.1.0",
            "transformers>=4.40.0",
            "peft>=0.7.0",
            "datasets>=2.16.0",
            "accelerate>=0.25.0",
            "bitsandbytes>=0.41.0",
            "sentence-transformers>=2.2.0",
            "scikit-learn>=1.3.0",
            "pandas>=2.1.0",
            "numpy>=1.24.0",
            "tqdm>=4.65.0",
            "tensorboard>=2.15.0",
            "jupyter>=1.0.0",
            "jupyterlab>=4.0.0",
            "matplotlib>=3.7.0",
            "seaborn>=0.12.0"
        ]
        
        req_path = self.training_dir / "requirements.txt"
        with open(req_path, "w") as f:
            f.write("\\n".join(requirements))
        
        print(f"üì¶ Requirements file created: {req_path}")
    
    def generate_setup_summary(self) -> Dict:
        """Generate setup summary"""
        model_info = get_model_info()
        
        summary = {
            "setup_timestamp": datetime.now().isoformat(),
            "project_root": self.project_root,
            "training_directory": str(self.training_dir),
            "model_configuration": {
                "base_model": self.config.base_model_id,
                "model_name": self.config.model_name,
                "max_sequence_length": self.config.max_sequence_length,
                "use_lora": self.config.use_lora,
                "use_4bit": self.config.use_4bit
            },
            "training_domains": model_info["training_domains"],
            "target_certifications": model_info["target_certifications"],
            "integration_points": model_info["integration_points"],
            "files_created": [
                "model_config.py",
                "data_collection.py", 
                "evaluation_metrics.py",
                "whis_cybersec_finetuning.ipynb",
                "train_whis.py",
                "launch_jupyter.sh",
                "requirements.txt"
            ]
        }
        
        return summary
    
    async def run_complete_setup(self):
        """Execute complete pipeline setup"""
        print("üõ°Ô∏è Whis Cybersecurity LLM Training Pipeline Setup")
        print("=" * 60)
        
        # Check system requirements
        if not self.check_dependencies():
            print("‚ùå Setup failed: Missing dependencies")
            return False
        
        self.check_gpu_availability()
        
        # Collect training data
        data_file = await self.collect_training_data()
        
        # Create latest symlink for training script
        latest_link = self.training_dir / "training_data" / "cybersec_training_data_latest.json"
        if latest_link.exists():
            latest_link.unlink()
        latest_link.symlink_to(Path(data_file).name)
        
        # Create training scripts
        self.create_training_script()
        self.create_jupyter_launcher()
        self.create_requirements_file()
        
        # Generate summary
        summary = self.generate_setup_summary()
        
        summary_file = self.training_dir / "setup_summary.json"
        with open(summary_file, "w") as f:
            json.dump(summary, f, indent=2)
        
        print("\\nüéØ Setup Complete!")
        print("=" * 40)
        print(f"üìÅ Training Directory: {self.training_dir}")
        print(f"üìä Training Data: {len(summary['files_created'])} files created")
        print(f"ü§ñ Model: {summary['model_configuration']['model_name']}")
        
        print("\\nüöÄ Next Steps:")
        print("1. Run Jupyter notebook:")
        print(f"   cd {self.training_dir} && ./launch_jupyter.sh")
        print("2. Or run training script directly:")
        print(f"   cd {self.training_dir} && python train_whis.py")
        print("3. Monitor training with TensorBoard:")
        print(f"   tensorboard --logdir {self.config.output_dir}")
        
        print("\\nüõ°Ô∏è Whis is ready for cybersecurity fine-tuning!")
        return True

async def main():
    """Main setup function"""
    pipeline = WhisTrainingPipeline()
    success = await pipeline.run_complete_setup()
    
    if success:
        print("‚úÖ Pipeline setup successful!")
        return 0
    else:
        print("‚ùå Pipeline setup failed!")
        return 1

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)