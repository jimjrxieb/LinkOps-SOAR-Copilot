#!/usr/bin/env python3
"""
ğŸ”„ CI-Triggered Index Refresh
============================
Automatically re-embeds changed files and gates updates with RAGAS evals.

Senior-level CI integration:
- Git hooks for detecting changes
- Incremental re-embedding (only changed files)
- RAGAS evaluation gates before promotion
- Rollback capabilities
- SLO enforcement (latency, faithfulness)
"""

import os
import sys
import json
import yaml
import hashlib
import subprocess
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import git

# Add parent directories to path
sys.path.append(str(Path(__file__).parent.parent))

from rag.repo_ingestion import RepoIngestionPipeline
from rag.hybrid_retrieval import HybridRetriever
from eval.run_eval import WhisEvaluator
import mlflow


class IndexRefreshPipeline:
    """CI-triggered index refresh with quality gates"""
    
    def __init__(self, config_path: str = "ai-training/ci/index_refresh.yaml"):
        self.config_path = Path(config_path)
        self.config = self._load_config()
        
        # Initialize components
        self.repo_path = Path(self.config["repository"]["path"])
        self.repo = git.Repo(self.repo_path)
        
        # State tracking
        self.state_file = Path("ai-training/ci/index_state.json")
        self.state = self._load_state()
        
    def _load_config(self) -> Dict[str, Any]:
        """Load CI configuration"""
        if not self.config_path.exists():
            default_config = {
                "repository": {
                    "path": ".",
                    "main_branch": "main",
                    "watch_patterns": [
                        "**/*.py", "**/*.js", "**/*.ts", "**/*.go", "**/*.java",
                        "**/*.md", "**/*.yaml", "**/*.json", "**/README*",
                        "**/Dockerfile", "**/docker-compose*", "**/*.toml"
                    ]
                },
                "triggers": {
                    "on_push_to_main": True,
                    "on_pr_merge": True,
                    "on_manual": True,
                    "schedule_cron": "0 2 * * *",  # Daily at 2 AM
                    "max_files_per_run": 100
                },
                "embedding": {
                    "batch_size": 10,
                    "timeout_seconds": 300,
                    "retry_attempts": 3
                },
                "evaluation": {
                    "ragas_enabled": True,
                    "golden_set_path": "ai-training/eval/benchmarks/rag_golden.jsonl",
                    "faithfulness_threshold": 0.75,
                    "context_precision_threshold": 0.70,
                    "context_recall_threshold": 0.70,
                    "latency_p95_threshold_ms": 3000,
                    "min_eval_samples": 50
                },
                "promotion": {
                    "staging_auto": True,
                    "production_manual": True,
                    "rollback_on_failure": True,
                    "keep_versions": 5
                },
                "monitoring": {
                    "slack_webhook": "${SLACK_WEBHOOK_URL}",
                    "email_alerts": "${CI_EMAIL_ALERTS}",
                    "mlflow_tracking": True
                }
            }
            
            self.config_path.parent.mkdir(parents=True, exist_ok=True)
            with open(self.config_path, 'w') as f:
                yaml.dump(default_config, f, indent=2)
                
            return default_config
            
        with open(self.config_path, 'r') as f:
            return yaml.safe_load(f)
            
    def _load_state(self) -> Dict[str, Any]:
        """Load index refresh state"""
        if self.state_file.exists():
            with open(self.state_file, 'r') as f:
                return json.load(f)
        else:
            return {
                "last_processed_commit": "",
                "last_successful_refresh": "",
                "index_versions": {},
                "evaluation_history": [],
                "rollback_points": []
            }
            
    def _save_state(self):
        """Save index refresh state"""
        self.state_file.parent.mkdir(parents=True, exist_ok=True)
        with open(self.state_file, 'w') as f:
            json.dump(self.state, f, indent=2)
            
    def detect_changes(self, since_commit: Optional[str] = None) -> Dict[str, List[Path]]:
        """Detect changed files since last refresh"""
        if not since_commit:
            since_commit = self.state["last_processed_commit"]
            
        if not since_commit:
            # First run - process all files
            print("ğŸ†• First run - processing all repository files")
            return {"all": list(self.repo_path.rglob("*"))}
            
        try:
            # Get changed files
            current_commit = self.repo.head.commit.hexsha
            
            if since_commit == current_commit:
                print("âœ… No changes detected")
                return {"changed": []}
                
            # Get diff between commits
            diff = self.repo.git.diff(f"{since_commit}..{current_commit}", name_only=True)
            changed_files = [Path(self.repo_path / f) for f in diff.split('\n') if f]
            
            # Filter by watch patterns
            watched_files = []
            watch_patterns = self.config["repository"]["watch_patterns"]
            
            for file_path in changed_files:
                if file_path.exists() and file_path.is_file():
                    if any(file_path.match(pattern) for pattern in watch_patterns):
                        watched_files.append(file_path)
                        
            print(f"ğŸ“ Detected {len(watched_files)} changed files to process")
            return {"changed": watched_files}
            
        except Exception as e:
            print(f"âš ï¸  Error detecting changes: {e}")
            return {"changed": []}
            
    def incremental_reembedding(self, changed_files: List[Path]) -> Dict[str, Any]:
        """Re-embed only changed files"""
        print(f"ğŸ”„ Starting incremental re-embedding for {len(changed_files)} files")
        
        # Initialize repo ingestion pipeline
        ingestion = RepoIngestionPipeline(str(self.repo_path))
        
        # Process changed files
        new_chunks = []
        
        for file_path in changed_files:
            try:
                # Determine file category
                if file_path.suffix in ['.py', '.js', '.ts', '.go', '.java', '.cpp', '.c']:
                    chunks = ingestion.chunk_code_file(file_path)
                else:
                    chunks = ingestion.chunk_docs_file(file_path)
                    
                new_chunks.extend(chunks)
                print(f"ğŸ“‚ Processed {file_path.name}: {len(chunks)} chunks")
                
            except Exception as e:
                print(f"âš ï¸  Failed to process {file_path}: {e}")
                
        # Update vector indexes
        embedding_results = self._update_vector_indexes(new_chunks)
        
        return {
            "processed_files": len(changed_files),
            "new_chunks": len(new_chunks),
            "embedding_results": embedding_results,
            "timestamp": datetime.now().isoformat()
        }
        
    def _update_vector_indexes(self, chunks: List[Any]) -> Dict[str, Any]:
        """Update vector indexes with new chunks"""
        print("ğŸ“Š Updating vector indexes...")
        
        try:
            # Initialize hybrid retriever to access vector stores
            retriever = HybridRetriever()
            
            results = {
                "code_chunks": 0,
                "docs_chunks": 0,
                "config_chunks": 0,
                "total_updated": 0
            }
            
            # Group chunks by type
            chunk_groups = {
                "code": [],
                "docs": [],
                "config": []
            }
            
            for chunk in chunks:
                chunk_type = getattr(chunk, 'chunk_type', 'docs')
                if chunk_type == 'code':
                    chunk_groups["code"].append(chunk)
                elif chunk_type == 'config':
                    chunk_groups["config"].append(chunk)
                else:
                    chunk_groups["docs"].append(chunk)
                    
            # Update each collection
            for group_type, group_chunks in chunk_groups.items():
                if not group_chunks:
                    continue
                    
                collection = retriever.vector_stores.get(group_type)
                if not collection:
                    continue
                    
                # Prepare data for ChromaDB
                ids = []
                documents = []
                embeddings = []
                metadatas = []
                
                for chunk in group_chunks:
                    # Generate chunk ID
                    chunk_id = hashlib.sha256(
                        f"{chunk.file_path}:{chunk.start_line}:{chunk.end_line}".encode()
                    ).hexdigest()[:16]
                    
                    # Get embeddings
                    chunk_dict = {
                        "content": chunk.content,
                        "chunk_type": chunk.chunk_type,
                        "language": chunk.language
                    }
                    chunk_embeddings = retriever.dual_embedder.embed_chunk(chunk_dict)
                    
                    # Use appropriate embedding
                    if group_type == "code" and "code" in chunk_embeddings:
                        embedding = chunk_embeddings["code"]
                    elif "prose" in chunk_embeddings:
                        embedding = chunk_embeddings["prose"]
                    else:
                        continue
                        
                    ids.append(chunk_id)
                    documents.append(chunk.content)
                    embeddings.append(embedding.tolist())
                    metadatas.append({
                        "file_path": chunk.file_path,
                        "symbol_name": chunk.symbol_name,
                        "symbol_type": chunk.symbol_type,
                        "start_line": chunk.start_line,
                        "end_line": chunk.end_line,
                        "language": chunk.language,
                        "commit_hash": chunk.commit_hash,
                        "module_path": chunk.module_path,
                        "chunk_type": chunk.chunk_type,
                        "last_modified": chunk.last_modified.isoformat()
                    })
                    
                # Upsert to collection (add or update)
                if ids:
                    collection.upsert(
                        ids=ids,
                        documents=documents,
                        embeddings=embeddings,
                        metadatas=metadatas
                    )
                    
                    results[f"{group_type}_chunks"] = len(ids)
                    results["total_updated"] += len(ids)
                    
                    print(f"ğŸ“Š Updated {group_type} index: {len(ids)} chunks")
                    
            return results
            
        except Exception as e:
            print(f"âŒ Failed to update vector indexes: {e}")
            return {"error": str(e)}
            
    def run_evaluation_gates(self) -> Dict[str, Any]:
        """Run RAGAS evaluation gates"""
        print("ğŸ§ª Running evaluation gates...")
        
        eval_config = self.config["evaluation"]
        
        if not eval_config["ragas_enabled"]:
            print("â­ï¸  RAGAS evaluation disabled")
            return {"passed": True, "reason": "evaluation_disabled"}
            
        try:
            # Initialize evaluator
            evaluator = WhisEvaluator()
            
            # Load golden set
            golden_set_path = Path(eval_config["golden_set_path"])
            if not golden_set_path.exists():
                print(f"âš ï¸  Golden set not found: {golden_set_path}")
                return {"passed": False, "reason": "golden_set_missing"}
                
            # Run RAG evaluation
            rag_metrics = evaluator.evaluate_rag(
                HybridRetriever(), 
                "rag_refresh_eval"
            )
            
            # Check thresholds
            gates_passed = True
            gate_results = {}
            
            # Faithfulness gate
            faithfulness = rag_metrics.get("faithfulness", 0)
            faithfulness_threshold = eval_config["faithfulness_threshold"]
            gate_results["faithfulness"] = {
                "value": faithfulness,
                "threshold": faithfulness_threshold,
                "passed": faithfulness >= faithfulness_threshold
            }
            if not gate_results["faithfulness"]["passed"]:
                gates_passed = False
                
            # Context precision gate
            context_precision = rag_metrics.get("context_precision", 0)
            precision_threshold = eval_config["context_precision_threshold"]
            gate_results["context_precision"] = {
                "value": context_precision,
                "threshold": precision_threshold,
                "passed": context_precision >= precision_threshold
            }
            if not gate_results["context_precision"]["passed"]:
                gates_passed = False
                
            # Context recall gate
            context_recall = rag_metrics.get("context_recall", 0)
            recall_threshold = eval_config["context_recall_threshold"]
            gate_results["context_recall"] = {
                "value": context_recall,
                "threshold": recall_threshold,
                "passed": context_recall >= recall_threshold
            }
            if not gate_results["context_recall"]["passed"]:
                gates_passed = False
                
            # Performance SLO check (simplified)
            # Would implement actual latency measurement
            gate_results["latency_p95"] = {
                "value": 2500,  # Placeholder
                "threshold": eval_config["latency_p95_threshold_ms"],
                "passed": True  # Placeholder
            }
            
            result = {
                "passed": gates_passed,
                "gates": gate_results,
                "metrics": rag_metrics,
                "timestamp": datetime.now().isoformat()
            }
            
            # Log to MLflow if enabled
            if self.config["monitoring"]["mlflow_tracking"]:
                with mlflow.start_run(run_name=f"index_refresh_{datetime.now().strftime('%Y%m%d_%H%M%S')}"):
                    mlflow.log_metrics(rag_metrics)
                    mlflow.log_params({
                        "evaluation_type": "index_refresh",
                        "gates_passed": gates_passed
                    })
                    
            print(f"ğŸ“Š Evaluation gates: {'âœ… PASSED' if gates_passed else 'âŒ FAILED'}")
            return result
            
        except Exception as e:
            print(f"âŒ Evaluation failed: {e}")
            return {"passed": False, "error": str(e)}
            
    def promote_index(self, stage: str = "staging") -> bool:
        """Promote index to staging/production"""
        print(f"ğŸš€ Promoting index to {stage}")
        
        try:
            # Create index version
            version = datetime.now().strftime("%Y%m%d_%H%M%S")
            current_commit = self.repo.head.commit.hexsha
            
            # Save version metadata
            version_metadata = {
                "version": version,
                "commit_hash": current_commit,
                "stage": stage,
                "created_at": datetime.now().isoformat(),
                "status": "active"
            }
            
            # Update state
            self.state["index_versions"][version] = version_metadata
            
            if stage == "staging":
                self.state["staging_version"] = version
            elif stage == "production":
                self.state["production_version"] = version
                
            self._save_state()
            
            print(f"âœ… Index promoted to {stage}: version {version}")
            return True
            
        except Exception as e:
            print(f"âŒ Failed to promote index: {e}")
            return False
            
    def rollback_index(self, stage: str, target_version: Optional[str] = None) -> bool:
        """Rollback index to previous version"""
        print(f"ğŸ”„ Rolling back {stage} index")
        
        try:
            if target_version:
                rollback_version = target_version
            else:
                # Find previous version
                versions = list(self.state["index_versions"].keys())
                if len(versions) < 2:
                    print("âŒ No previous version available for rollback")
                    return False
                    
                current_version = self.state.get(f"{stage}_version")
                if current_version in versions:
                    current_idx = versions.index(current_version)
                    if current_idx > 0:
                        rollback_version = versions[current_idx - 1]
                    else:
                        print("âŒ No previous version available")
                        return False
                else:
                    rollback_version = versions[-2]  # Second latest
                    
            # Perform rollback (simplified - would involve actual index restoration)
            self.state[f"{stage}_version"] = rollback_version
            self.state["index_versions"][rollback_version]["status"] = "active"
            
            # Record rollback
            self.state["rollback_points"].append({
                "timestamp": datetime.now().isoformat(),
                "stage": stage,
                "from_version": self.state.get(f"{stage}_version"),
                "to_version": rollback_version
            })
            
            self._save_state()
            
            print(f"âœ… Rolled back {stage} to version {rollback_version}")
            return True
            
        except Exception as e:
            print(f"âŒ Rollback failed: {e}")
            return False
            
    def run_refresh_pipeline(self, trigger: str = "manual") -> Dict[str, Any]:
        """Run complete index refresh pipeline"""
        print(f"ğŸ”„ Starting index refresh pipeline (trigger: {trigger})")
        
        start_time = datetime.now()
        pipeline_results = {
            "trigger": trigger,
            "start_time": start_time.isoformat(),
            "stages": {}
        }
        
        # Stage 1: Detect changes
        print("\nğŸ“ Stage 1: Detecting changes")
        changes = self.detect_changes()
        pipeline_results["stages"]["change_detection"] = {
            "files_changed": len(changes.get("changed", [])),
            "success": True
        }
        
        if not changes.get("changed"):
            print("âœ… No changes detected - pipeline complete")
            pipeline_results["result"] = "no_changes"
            return pipeline_results
            
        # Stage 2: Incremental re-embedding
        print("\nğŸ”„ Stage 2: Incremental re-embedding")
        try:
            embedding_results = self.incremental_reembedding(changes["changed"])
            pipeline_results["stages"]["embedding"] = {
                "success": True,
                **embedding_results
            }
        except Exception as e:
            print(f"âŒ Embedding failed: {e}")
            pipeline_results["stages"]["embedding"] = {"success": False, "error": str(e)}
            pipeline_results["result"] = "embedding_failed"
            return pipeline_results
            
        # Stage 3: Evaluation gates
        print("\nğŸ§ª Stage 3: Running evaluation gates")
        eval_results = self.run_evaluation_gates()
        pipeline_results["stages"]["evaluation"] = eval_results
        
        if not eval_results.get("passed", False):
            print("âŒ Evaluation gates failed - not promoting")
            
            # Rollback if configured
            if self.config["promotion"]["rollback_on_failure"]:
                print("ğŸ”„ Rolling back due to failed gates")
                self.rollback_index("staging")
                
            pipeline_results["result"] = "evaluation_failed"
            return pipeline_results
            
        # Stage 4: Promotion
        print("\nğŸš€ Stage 4: Promoting index")
        if self.config["promotion"]["staging_auto"]:
            staging_success = self.promote_index("staging")
            pipeline_results["stages"]["staging_promotion"] = {"success": staging_success}
            
            if staging_success and self.config["promotion"].get("production_auto", False):
                prod_success = self.promote_index("production")
                pipeline_results["stages"]["production_promotion"] = {"success": prod_success}
                
        # Update state
        current_commit = self.repo.head.commit.hexsha
        self.state["last_processed_commit"] = current_commit
        self.state["last_successful_refresh"] = datetime.now().isoformat()
        
        # Add to evaluation history
        self.state["evaluation_history"].append({
            "timestamp": datetime.now().isoformat(),
            "trigger": trigger,
            "commit": current_commit,
            "gates_passed": eval_results.get("passed", False),
            "metrics": eval_results.get("metrics", {})
        })
        
        # Keep only recent history
        if len(self.state["evaluation_history"]) > 50:
            self.state["evaluation_history"] = self.state["evaluation_history"][-50:]
            
        self._save_state()
        
        pipeline_results["result"] = "success"
        pipeline_results["end_time"] = datetime.now().isoformat()
        pipeline_results["duration_seconds"] = (datetime.now() - start_time).total_seconds()
        
        print(f"\nğŸ‰ Index refresh pipeline completed successfully!")
        print(f"â±ï¸  Duration: {pipeline_results['duration_seconds']:.1f} seconds")
        
        return pipeline_results


def main():
    """CLI for index refresh pipeline"""
    import argparse
    
    parser = argparse.ArgumentParser(description="CI-triggered index refresh")
    parser.add_argument("--config", default="ai-training/ci/index_refresh.yaml")
    parser.add_argument("--trigger", default="manual", help="Trigger type")
    parser.add_argument("--since-commit", help="Process changes since commit")
    parser.add_argument("--rollback", help="Rollback stage to previous version")
    parser.add_argument("--stage", default="staging", help="Target stage for rollback")
    
    args = parser.parse_args()
    
    # Initialize pipeline
    pipeline = IndexRefreshPipeline(args.config)
    
    if args.rollback:
        # Perform rollback
        success = pipeline.rollback_index(args.rollback)
        sys.exit(0 if success else 1)
    else:
        # Run refresh pipeline
        results = pipeline.run_refresh_pipeline(args.trigger)
        
        # Exit with appropriate code
        if results["result"] == "success":
            print("âœ… Pipeline completed successfully")
            sys.exit(0)
        else:
            print(f"âŒ Pipeline failed: {results['result']}")
            sys.exit(1)


if __name__ == "__main__":
    main()